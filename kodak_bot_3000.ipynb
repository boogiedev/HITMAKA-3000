{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto Reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Modules\n",
    "import csv\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from pprint import pprint\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from math import log\n",
    "import re\n",
    "from gensim import corpora\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# NLTK Modules\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('tagsets')\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import chunk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Import Custom Modules\n",
    "from src.data_cleaner import *\n",
    "from src.dummy_words import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use lyricsgenius module\n",
    "import lyricsgenius as genius\n",
    "# Import Credentials\n",
    "from src.credentials.keys import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape web to get Kodak Black lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set Target Artist\n",
    "# trg_artist = 'Kodak Black'\n",
    "# # Create API Instance\n",
    "# api = genius.Genius(client_token)\n",
    "# # Search Artist Information\n",
    "# artist = api.search_artist(trg_artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Lyrics from Artist\n",
    "# aux = artist.save_lyrics(filename='lyrics.json', overwrite=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in recently created JSON\n",
    "titles = None\n",
    "with open('data/lyrics.json') as jsonfile:\n",
    "    data = json.load(jsonfile)\n",
    "    songs = data['songs']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Title of songs\n",
    "titles = [song['title'] for song in songs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Intro]\\nGlee\\nSouthside\\nAyy, lil\\' Metro on that beat\\n\\n[Chorus]\\nLil\\' Kodak, they don\\'t like to see you winnin\\'\\nThey wanna see you in the penitentiary\\nI need me a lil\\' baby who gon\\' listen\\nGirl, I don\\'t wanna be the one you iggin\\'\\nMy mama told me, \"Boy, make good decisions\"\\nRight now I gotta keep a tun'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Lyrics from each song\n",
    "lyrics = [song['lyrics'] for song in songs]\n",
    "\n",
    "# Taking a look at what a single song looks like (truncated)\n",
    "lyrics[0][:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Lyrics\n",
    "\n",
    "> Remove headers from each lyric set and standardize casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nglee\\nsouthside\\nayy, lil\\' metro on that beat\\n\\nlil\\' kodak, they don\\'t like to see you winnin\\'\\nthey wanna see you in the penitentiary\\ni need me a lil\\' baby who gon\\' listen\\ngirl, i don\\'t wanna be the one you iggin\\'\\nmy mama told me, \"boy, make good decisions\"\\nright now i gotta keep a tunnel vision\\ni nee'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use custom function to remove headers from each set of lyrics\n",
    "clean_lyrics = [remove_headers(lyric).lower() for lyric in lyrics]\n",
    "clean_lyrics[0][:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remove new-line breaks and escapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' glee southside ayy, lil metro on that beat  lil kodak, they dont like to see you winnin they wanna see you in the penitentiary i need me a lil baby who gon listen girl, i dont wanna be the one you iggin my mama told me, \"boy, make good decisions\" right now i gotta keep a tunnel vision i need me a lil baby who gon listen girl, i dont wanna be the one you iggin lil kodak, they dont like to see you winnin they wanna see you in the penitentiary i need me a lil baby who gon listen girl, i dont wanna be the one you iggin my mama told me, \"boy, make good decisions\" right now i gotta keep a tunnel vision they sendin all my homies on a mission and i aint tryna miss out on these millions  i jumped up out the wraith, kodak bought a wraith i get any girl i want, any girl i want she want me to save the day, but i aint got a cape she wanna see me every day, she wanna be my bae that money make me cum, it make me fornicate, uh im the shit, i need some toilet pap-er follow my player rules, then we gon be okay on the real i need a bih who gon cooperate  lil kodak, they dont like to see you winnin they wanna see you in the penitentiary i need me a lil baby who gon listen girl, i dont wanna be the one you iggin my mama told me, \"boy, make good decisions\" right now i gotta keep a tunnel vision i need me a lil baby who gon listen girl, i dont wanna be the one you iggin lil kodak, they dont like to see you winnin they wanna see you in the penitentiary i need me a lil baby who gon listen girl, i dont wanna be the one you iggin my mama told me, \"boy, make good decisions\" right now i gotta keep a tunnel vision they sendin all my homies on a mission and i aint tryna miss out on these millions  i told you niggas im gon be that fuckin nigga everything be good til you doin better than em they wanna see a nigga shot or see me in the system all you niggas out here poison like snake venom codeine in my liver, rockin balenciaga denim im booted even when im sober, i dont need a jigga they sendin all my niggas on a fuckin journey i told that baby to come over cause im fuckin horny my mama told me, \"kill these niggas, son, keep it goin!\" im thug to the bone, but im still her baby boy lil kodak, boy, they hate to see you in a foreign they miss when you was in that yota kickin doors  lil kodak, they dont like to see you winnin they wanna see you in the penitentiary i need me a lil baby who gon listen girl, i dont wanna be the one you iggin my mama told me, \"boy, make good decisions\" right now i gotta keep a tunnel vision i need me a lil baby who gon listen girl, i dont wanna be the one you iggin lil kodak, they dont like to see you winnin they wanna see you in the penitentiary i need me a lil baby who gon listen girl, i dont wanna be the one you iggin my mama told me, \"boy, make good decisions\" right now i gotta keep a tunnel vision they sendin all my homies on a mission and i aint tryna miss out on these millions'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace new-line breaks and escape characters\n",
    "clean_lyrics = [lyric.replace('\\n', ' ') for lyric in clean_lyrics]\n",
    "clean_lyrics = [lyric.replace('\\'', '') for lyric in clean_lyrics]\n",
    "\n",
    "clean_lyrics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tokenize words manually (NLTK proven uneffective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glee',\n",
       " 'southside',\n",
       " 'ayy',\n",
       " 'lil',\n",
       " 'metro',\n",
       " 'on',\n",
       " 'that',\n",
       " 'beat',\n",
       " 'lil',\n",
       " 'kodak']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize Words\n",
    "tokens = [lyric.split() for lyric in clean_lyrics]\n",
    "tokens = [[re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem) for elem in token] for token in tokens]\n",
    "tokens = [[word for word in token if word] for token in tokens]\n",
    "tokens[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glee',\n",
       " 'southside',\n",
       " 'ayy',\n",
       " 'lil',\n",
       " 'metro',\n",
       " 'beat',\n",
       " 'lil',\n",
       " 'kodak',\n",
       " 'dont',\n",
       " 'like']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [[word for word in token if word not in stop_words] for token in tokens] \n",
    "tokens[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking into Stemming or Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_song = list(set(tokens[0]))\n",
    "# Create Porter, Snowball, WordNet Objects\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "# Get functions from each object\n",
    "porter_func = porter.stem\n",
    "snowball_func = snowball.stem\n",
    "wordnet_func = wordnet.lemmatize\n",
    "\n",
    "# Create lambda func to easily apply func to each token\n",
    "get_root = lambda tokens, func: [func(token) for token in tokens] \n",
    "\n",
    "# Get Tokens for each type of processor\n",
    "porter_tokens = list(map(porter_func, first_song))\n",
    "snowball_tokens = list(map(snowball_func, first_song)) \n",
    "wordnet_tokens = list(map(wordnet_func, first_song)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            WORD |           PORTER |         SNOWBALL |       LEMMATIZER |\n",
      "    penitentiary |     penitentiari |     penitentiari |     penitentiary |\n",
      "            baby |             babi |             babi |             baby |\n",
      "         codeine |           codein |           codein |          codeine |\n",
      "           cause |             caus |             caus |            cause |\n",
      "          homies |             homi |             homi |           homies |\n",
      "       cooperate |           cooper |           cooper |        cooperate |\n",
      "       fornicate |           fornic |           fornic |        fornicate |\n",
      "          booted |             boot |             boot |           booted |\n",
      "           horny |            horni |            horni |            horny |\n",
      "           every |            everi |            everi |            every |\n",
      "      everything |          everyth |          everyth |       everything |\n",
      "             ayy |              ayi |              ayi |              ayy |\n",
      "          jumped |             jump |             jump |           jumped |\n",
      "       southside |         southsid |         southsid |        southside |\n",
      "       decisions |            decis |            decis |         decision |\n"
     ]
    }
   ],
   "source": [
    "## Print the stemmed and lemmatized words from the target document\n",
    "print(\"%16s | %16s | %16s | %16s |\" % (\"WORD\", \"PORTER\", \"SNOWBALL\", \"LEMMATIZER\"))\n",
    "for i in range(min(len(porter_tokens), len(snowball_tokens), len(wordnet_tokens))):\n",
    "    p, s, w = porter_tokens[i], snowball_tokens[i], wordnet_tokens[i]\n",
    "    if len(set((p, s, w))) != 1:\n",
    "        print(\"%16s | %16s | %16s | %16s |\" % (first_song[i], p, s, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOC2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Doc2Vec Model\n",
    "dv_model = Doc2Vec(\n",
    "    alpha=0.025,\n",
    "    min_alpha=0.025,\n",
    "    workers=15, \n",
    "    min_count=2,\n",
    "    window=10,\n",
    "    vector_size=300,\n",
    "    epochs=20,\n",
    "    sample=0.001,\n",
    "    negative=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***alpha*** is the initial learning rate. A very intuitive explanation for learning rate can be found here. Essentially, the learning rate is, as stated in the link, \"how quickly a network abandons old beliefs for new ones.\"\n",
    "- ***min_alpha*** is exactly what it sounds like, the minimum alpha can be, which we reduce after every epoch.\n",
    "- ***workers*** is the number of threads used to train the model.\n",
    "- ***min_count*** specifies a term frequency that must be met for a word to be considered by the model.\n",
    "- ***window*** is how many words in front and behind the input word should be considered when determining context.\n",
    "- ***size*** is the number of dimensions. Unlike most numerical datasets that have 2 dimensions, text data can have hundreds or even more.\n",
    "- ***iter*** is the number of iterations, the number of times the training set passes through the algorithm.\n",
    "- ***sample*** is the downsampling rate. Words representing more than this will be eligible for downsampling.\n",
    "- ***negative*** is the negative sampling rate. 0 means update all weights in the output layer of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Sentences via TaggedDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TaggedDocument objects with song tokens and tag (song number)\n",
    "sentences = [TaggedDocument(\n",
    "             words=document,\n",
    "             tags=['%s | %s' % (i, titles[i])]) \n",
    "             for i, document in enumerate(tokens)]\n",
    "# sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training and Vocab Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model Vocab\n",
    "dv_model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    dv_model.train(sentences, total_examples=dv_model.corpus_count, epochs=dv_model.epochs)\n",
    "    dv_model.alpha -= 0.002  # decrease the learning rate\n",
    "    dv_model.min_alpha = dv_model.alpha  # fix the learning rate, no decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_model.save('data/kodak_lyrics.doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model to Test Save\n",
    "dv_model = Doc2Vec.load('data/kodak_lyrics.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at word similarity and association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arrest', 0.3720146715641022),\n",
       " ('found', 0.3172382712364197),\n",
       " ('infinity', 0.29102620482444763),\n",
       " ('price', 0.2906968593597412),\n",
       " ('paid', 0.28518110513687134),\n",
       " ('dip', 0.264195054769516),\n",
       " ('thots', 0.2627524733543396),\n",
       " ('winter', 0.25071802735328674),\n",
       " ('manuvering', 0.2504286766052246),\n",
       " ('gone', 0.2497052550315857)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# House\n",
    "dv_model.wv.most_similar('house')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('whip', 0.28635820746421814),\n",
       " ('anyways', 0.2713841199874878),\n",
       " ('booger', 0.25949209928512573),\n",
       " ('woo', 0.2574138641357422),\n",
       " ('tiger', 0.23631080985069275),\n",
       " ('protest', 0.23347043991088867),\n",
       " ('soo', 0.2253037691116333),\n",
       " ('legendary', 0.2240905463695526),\n",
       " ('qp', 0.22198450565338135),\n",
       " ('fruit', 0.22125600278377533)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coupe\n",
    "dv_model.wv.most_similar('coupe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
