{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto Reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Modules\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from pprint import pprint\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from math import log\n",
    "import re\n",
    "\n",
    "\n",
    "# NLTK Modules\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('tagsets')\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import chunk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Import Custom Modules\n",
    "from src.data_cleaner import *\n",
    "from src.dummy_words import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "clean_df = pd.read_pickle('data/clean_data.pkl')\n",
    "# Rid Period from clean text\n",
    "clean_df['clean_text'] = clean_df['clean_text'].apply(lambda x: \"\".join(x.split(\".\")))\n",
    "# clean_df['clean_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>featured</th>\n",
       "      <th>rank</th>\n",
       "      <th>year</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>lyrics_state</th>\n",
       "      <th>song_id</th>\n",
       "      <th>lyrics_owner_id</th>\n",
       "      <th>primary_artist_url</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_stop</th>\n",
       "      <th>token_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>See You Again</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>[its been a long day without you my friend, an...</td>\n",
       "      <td>True</td>\n",
       "      <td>720401</td>\n",
       "      <td>341761</td>\n",
       "      <td>https://genius.com/artists/Wiz-khalifa</td>\n",
       "      <td>its been a long day without you my friend and ...</td>\n",
       "      <td>[its been a long day without you my friend., a...</td>\n",
       "      <td>[its, been, a, long, day, without, you, my, fr...</td>\n",
       "      <td>[long, day, without, friend, ill, tell, see, w...</td>\n",
       "      <td>[all, as, i, be, out, im, look, ill, weve, whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trap Queen</td>\n",
       "      <td>Fetty Wap</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>[rgf productions, remy boyz yahah, 1738 ayy, ,...</td>\n",
       "      <td>True</td>\n",
       "      <td>496445</td>\n",
       "      <td>104344</td>\n",
       "      <td>https://genius.com/artists/Fetty-wap</td>\n",
       "      <td>rgf productions remy boyz yahah 1738 ayy im li...</td>\n",
       "      <td>[rgf productions., remy boyz yahah., 1738 ayy....</td>\n",
       "      <td>[rgf, productions, remy, boyz, yahah, 1738, ay...</td>\n",
       "      <td>[rgf, productions, remy, boyz, yahah, 1738, ay...</td>\n",
       "      <td>[run, lambos, cause, introduced, all, necklace...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            song        artist  featured  rank  year  \\\n",
       "0  See You Again  Wiz Khalifa        NaN     1  2015   \n",
       "1     Trap Queen     Fetty Wap       NaN     2  2015   \n",
       "\n",
       "                                              lyrics  lyrics_state  song_id  \\\n",
       "0  [its been a long day without you my friend, an...          True   720401   \n",
       "1  [rgf productions, remy boyz yahah, 1738 ayy, ,...          True   496445   \n",
       "\n",
       "   lyrics_owner_id                      primary_artist_url  \\\n",
       "0           341761  https://genius.com/artists/Wiz-khalifa   \n",
       "1           104344    https://genius.com/artists/Fetty-wap   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  its been a long day without you my friend and ...   \n",
       "1  rgf productions remy boyz yahah 1738 ayy im li...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [its been a long day without you my friend., a...   \n",
       "1  [rgf productions., remy boyz yahah., 1738 ayy....   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [its, been, a, long, day, without, you, my, fr...   \n",
       "1  [rgf, productions, remy, boyz, yahah, 1738, ay...   \n",
       "\n",
       "                                         tokens_stop  \\\n",
       "0  [long, day, without, friend, ill, tell, see, w...   \n",
       "1  [rgf, productions, remy, boyz, yahah, 1738, ay...   \n",
       "\n",
       "                                           token_set  \n",
       "0  [all, as, i, be, out, im, look, ill, weve, whe...  \n",
       "1  [run, lambos, cause, introduced, all, necklace...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See Current Features\n",
    "# clean_df.columns\n",
    "clean_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing: Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Getting TERM FREQUENCY\n",
    "The number of times a term occurs in a specific document: \n",
    "\n",
    "$tf(term,document) = \\frac{\\# \\ of \\ times \\ a \\ term \\ appears \\ in \\ a \\ document}{\\#\\ of\\ terms\\ in\\ the\\ document|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Features to DataFrame of Term Occurences\n",
    "clean_df['term_occurences'] = clean_df['tokens'].apply(lambda x: Counter(x))\n",
    "# clean_df['term_occurences'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deciding to use the tokens where the stop-words were NOT filtered out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Features to DataFrame of Term Frequency\n",
    "clean_df['term_frequency'] = [{k: (v / float(len(clean_df['tokens'].iloc[i])))\n",
    "                       for k, v in clean_df['term_occurences'].iloc[i].items()} for i in range(len(clean_df['term_occurences']))]\n",
    "# clean_df['term_frequency'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Getting DOCUMENT FREQUENCY\n",
    "\n",
    "$df(term,corpus) = \\frac{ \\# \\ of \\ documents \\ that \\ contain \\ a \\ term}{ \\# \\ of \\ documents \\ in \\ the \\ corpus}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Features to DataFrame of Doc Occurences\n",
    "doc_occ = Counter([word for bow in clean_df['tokens'] for word in set(bow)])\n",
    "# doc_occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Features to DataFrame of Term Frequency\n",
    "doc_freq =  {k: (v / float(len(clean_df['tokens'])))\n",
    "            for k, v in doc_occ.items()}\n",
    "# doc_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### TFIDF vector\n",
    "\n",
    "$df(term,corpus) = \\frac{ \\# \\ of \\ documents \\ that \\ contain \\ a \\ term}{ \\# \\ of \\ documents \\ in \\ the \\ corpus}$\n",
    "\n",
    "The inverse document frequency is defined in terms of the document frequency as\n",
    "\n",
    "$idf(term,corpus) = \\log{\\frac{1}{df(term,corpus)}}$.\n",
    "\n",
    "TF-IDF is an acronym for the product of two parts: the term frequency tf and what is called the inverse document frequency idf. The term frequency is just the counts in a term frequency vector. \n",
    "\n",
    "tf-idf $ = tf(term,document) * idf(term,corpus)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = TfidfVectorizer()\n",
    "vec = tf_vectorizer.fit_transform(clean_df['clean_text'])\n",
    "vector_df_tf = pd.DataFrame(vec.toarray().transpose(),\n",
    "                         index=tf_vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
