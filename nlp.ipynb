{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto Reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import csv\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from pprint import pprint\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from math import log\n",
    "import re\n",
    "import os\n",
    "from gensim import corpora\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLTK Modules\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('tagsets')\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import chunk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Import Custom Modules\n",
    "from src.data_cleaner import *\n",
    "from src.dummy_words import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "clean_df = pd.read_pickle('data/clean_data.pkl')\n",
    "# Rid Period from clean text\n",
    "clean_df['clean_text'] = clean_df['clean_text'].apply(lambda x: \"\".join(x.split(\".\")))\n",
    "# clean_df['clean_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Current Features\n",
    "# clean_df.columns\n",
    "clean_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing: Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Getting TERM FREQUENCY\n",
    "The number of times a term occurs in a specific document: \n",
    "\n",
    "$tf(term,document) = \\frac{\\# \\ of \\ times \\ a \\ term \\ appears \\ in \\ a \\ document}{\\#\\ of\\ terms\\ in\\ the\\ document|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Features to DataFrame of Term Occurences\n",
    "clean_df['term_occurences'] = clean_df['tokens'].apply(lambda x: Counter(x))\n",
    "# clean_df['term_occurences'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deciding to use the tokens where the stop-words were NOT filtered out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Features to DataFrame of Term Frequency\n",
    "clean_df['term_frequency'] = [{k: (v / float(len(clean_df['tokens'].iloc[i])))\n",
    "                       for k, v in clean_df['term_occurences'].iloc[i].items()} for i in range(len(clean_df['term_occurences']))]\n",
    "# clean_df['term_frequency'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Getting DOCUMENT FREQUENCY\n",
    "\n",
    "$df(term,corpus) = \\frac{ \\# \\ of \\ documents \\ that \\ contain \\ a \\ term}{ \\# \\ of \\ documents \\ in \\ the \\ corpus}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Features to DataFrame of Doc Occurences\n",
    "doc_occ = Counter([word for bow in clean_df['tokens'] for word in set(bow)])\n",
    "# doc_occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Features to DataFrame of Term Frequency\n",
    "doc_freq =  {k: (v / float(len(clean_df['tokens'])))\n",
    "            for k, v in doc_occ.items()}\n",
    "# doc_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### TFIDF vector\n",
    "\n",
    "$df(term,corpus) = \\frac{ \\# \\ of \\ documents \\ that \\ contain \\ a \\ term}{ \\# \\ of \\ documents \\ in \\ the \\ corpus}$\n",
    "\n",
    "The inverse document frequency is defined in terms of the document frequency as\n",
    "\n",
    "$idf(term,corpus) = \\log{\\frac{1}{df(term,corpus)}}$.\n",
    "\n",
    "TF-IDF is an acronym for the product of two parts: the term frequency tf and what is called the inverse document frequency idf. The term frequency is just the counts in a term frequency vector. \n",
    "\n",
    "tf-idf $ = tf(term,document) * idf(term,corpus)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = TfidfVectorizer()\n",
    "vec = tf_vectorizer.fit_transform(clean_df['clean_text'])\n",
    "vector_df_tf = pd.DataFrame(vec.toarray().transpose(),\n",
    "                         index=tf_vectorizer.get_feature_names())\n",
    "vector_df_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer()\n",
    "vec = count_vec.fit_transform(clean_df['clean_text'])\n",
    "vector_df_cnt = pd.DataFrame(vec.toarray().transpose(),\n",
    "                         index=count_vec.get_feature_names())\n",
    "vector_df_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus\n",
    "corpus = clean_df['clean_text'].tolist()\n",
    "tokens = [word_tokenize(doc) for doc in corpus]\n",
    "tokens_stop = clean_df['tokens_stop'].tolist() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Yearly Corpus\n",
    "grouped = clean_df.groupby('year')\n",
    "\n",
    "group_2019 = clean_df.iloc[grouped.groups[2019]].copy()\n",
    "group_2018 = clean_df.iloc[grouped.groups[2018]].copy()\n",
    "group_2017 = clean_df.iloc[grouped.groups[2017]].copy()\n",
    "group_2016 = clean_df.iloc[grouped.groups[2016]].copy()\n",
    "group_2015 = clean_df.iloc[grouped.groups[2015]].copy()\n",
    "group_list = [group_2015, group_2016, group_2017, group_2018, group_2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0][:100], tokens[0][:5], tokens_stop[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at words by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = ['im', 'thats', 'ya', 'though', 'yeah']\n",
    "custom_stop = ['yeah', 'like', 'got', '2018', 'know', 'get', 'aint', 'ayy', 'go', 'na', 'back', 'one', 'gon', 'make', 'wan', 'thats', 'need', 'oh', 'see', 'feat', 'ooh', 'said', 'way', \"2017\"] + stops\n",
    "# Join the different processed titles together\n",
    "long_string = ','.join([\",\".join(tokens) for tokens in tokens_stop])\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"black\", max_words=500, contour_width=3,  width=800, height=400, stopwords=custom_stop)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Most Common Words from CounterVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer()\n",
    "vec = count_vec.fit_transform([\" \".join(set_) for set_ in clean_df['tokens_stop']])\n",
    "vector_df_cnt = pd.DataFrame(vec.toarray().transpose(),\n",
    "                         index=count_vec.get_feature_names())\n",
    "\n",
    "# Visualise the 10 most common words\n",
    "plot_10_most_common_words(vec, count_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot again after removing custom stop words (yeah, like, got, 2018, know, get, aint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer()\n",
    "vec = count_vec.fit_transform([\" \".join([x for x in set_ if x not in custom_stop]) for set_ in clean_df['tokens_stop']])\n",
    "vector_df_cnt = pd.DataFrame(vec.toarray().transpose(),\n",
    "                         index=count_vec.get_feature_names())\n",
    "\n",
    "# Visualise the 10 most common words\n",
    "plot_10_most_common_words(vec, count_vec, '10 Most Common Words w/ Custom Stops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# fig=plt.figure(figsize=(10,7))\n",
    "# columns = 3\n",
    "# rows = 2\n",
    "# a=np.random.rand(2,3)\n",
    "# for i in range(1, 6):\n",
    "#     fig.add_subplot(rows, columns, i)\n",
    "#     plt.plot(a)### what you want you can plot  \n",
    "\n",
    "for year, group, i in zip([2015, 2016, 2017, 2018, 2019], group_list, range(1,6)):\n",
    "    count_vec = CountVectorizer()\n",
    "    vec = count_vec.fit_transform([\" \".join([x for x in set_ if x not in custom_stop]) for set_ in group['tokens_stop']])\n",
    "    vector_df_cnt = pd.DataFrame(vec.toarray().transpose(),\n",
    "                             index=count_vec.get_feature_names())\n",
    "    words = count_vec.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in vec:\n",
    "        total_counts+=t.toarray()[0]\n",
    "\n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "\n",
    "    fig.add_subplot(2, 3, i)\n",
    "    sns.set_context(\"notebook\", font_scale=1.55, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette='husl')\n",
    "    plt.title(year)\n",
    "    plt.xticks(x_pos, words, rotation=90) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF Non Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
