{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto Reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Modules\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from pprint import pprint\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from math import log\n",
    "import re\n",
    "\n",
    "\n",
    "# NLTK Modules\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('tagsets')\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import chunk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Import Custom Modules\n",
    "from src.data_cleaner import *\n",
    "from src.dummy_words import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Scraping Program\n",
    "# !python src/web_scraper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intake Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>featured</th>\n",
       "      <th>rank</th>\n",
       "      <th>year</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>lyrics_state</th>\n",
       "      <th>song_id</th>\n",
       "      <th>lyrics_owner_id</th>\n",
       "      <th>primary_artist_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thrift Shop</td>\n",
       "      <td>Macklemore &amp; Ryan Lewis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>\"Hey, Macklemore, can we go thrift shopping?\"\\...</td>\n",
       "      <td>True</td>\n",
       "      <td>86538</td>\n",
       "      <td>3928</td>\n",
       "      <td>https://genius.com/artists/Macklemore-and-ryan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Can't Hold Us</td>\n",
       "      <td>Macklemore &amp; Ryan Lewis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>Hey, hey, hey\\nGood to see you\\nCome on, dude,...</td>\n",
       "      <td>True</td>\n",
       "      <td>57234</td>\n",
       "      <td>37383</td>\n",
       "      <td>https://genius.com/artists/Macklemore-and-ryan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Holy Grail</td>\n",
       "      <td>Jay Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>You'd take the clothes off my back and I'd let...</td>\n",
       "      <td>True</td>\n",
       "      <td>177832</td>\n",
       "      <td>104344</td>\n",
       "      <td>https://genius.com/artists/Jay-z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           song                    artist  featured  rank  year  \\\n",
       "0           0    Thrift Shop  Macklemore & Ryan Lewis        NaN     1  2000   \n",
       "1           1  Can't Hold Us  Macklemore & Ryan Lewis        NaN     2  2000   \n",
       "2           2     Holy Grail                    Jay Z        NaN     3  2000   \n",
       "\n",
       "                                              lyrics  lyrics_state  song_id  \\\n",
       "0  \"Hey, Macklemore, can we go thrift shopping?\"\\...          True    86538   \n",
       "1  Hey, hey, hey\\nGood to see you\\nCome on, dude,...          True    57234   \n",
       "2  You'd take the clothes off my back and I'd let...          True   177832   \n",
       "\n",
       "   lyrics_owner_id                                 primary_artist_url  \n",
       "0             3928  https://genius.com/artists/Macklemore-and-ryan...  \n",
       "1            37383  https://genius.com/artists/Macklemore-and-ryan...  \n",
       "2           104344                   https://genius.com/artists/Jay-z  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Data\n",
    "# data = pd.read_csv('data/data.csv')\n",
    "data = pd.read_csv('data/all_data.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 11 columns):\n",
      "Unnamed: 0            1000 non-null int64\n",
      "song                  1000 non-null object\n",
      "artist                1000 non-null object\n",
      "featured              0 non-null float64\n",
      "rank                  1000 non-null int64\n",
      "year                  1000 non-null int64\n",
      "lyrics                1000 non-null object\n",
      "lyrics_state          1000 non-null bool\n",
      "song_id               1000 non-null int64\n",
      "lyrics_owner_id       1000 non-null int64\n",
      "primary_artist_url    986 non-null object\n",
      "dtypes: bool(1), float64(1), int64(5), object(4)\n",
      "memory usage: 63.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[data['lyrics'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                            0\n",
       "song                                                        Thrift Shop\n",
       "artist                                         Macklemore & Ryan Lewis \n",
       "featured                                                            NaN\n",
       "rank                                                                  1\n",
       "year                                                               2000\n",
       "lyrics                \"Hey, Macklemore, can we go thrift shopping?\"\\...\n",
       "lyrics_state                                                       True\n",
       "song_id                                                           86538\n",
       "lyrics_owner_id                                                    3928\n",
       "primary_artist_url    https://genius.com/artists/Macklemore-and-ryan...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global Variables\n",
    "song_idx = 0\n",
    "data.iloc[song_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating pipeline for **indexing** song lyrics (document).\n",
    "\n",
    "This will lead to **indexing** which creates a **signature** (vector) for each document.\n",
    "\n",
    "Then, the **signatures** will be used for relating documents one to the other (and find out similar clusters of documents), or for mining underlying relations between concepts.\n",
    "\n",
    "<img src=\"media/text-pipeline.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bags of Words: Lyric String Example\n",
    "> With the intake lyric data, it seems that there needs to be a couple things cleaned. Casing, punctuation, and new-lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lyric String Passover 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Hey, Macklemore, can we go thrift shopping?\"\\nWhat what, what, what\\nWhat what, what, what\\nWhat what, what, what\\nWhat what, what, what\\nWhat what, what, what\\nBada, bada, bada doo da\\nWhat what, what, what\\nBada, bada, bada doo da\\nWhat what, what, what\\nBada, bada, bada doo da\\nBada, bada, bada doo da\\nBada, bada, bada doo da\\nBada, bada, bada doo da\\nBada, bada, bada doo da\\n\\nI\\'m gonna pop some tags\\nOnly got 20 dollars in my pocket\\nI\\'m, I\\'m, I\\'m huntin\\', lookin\\' for a come up\\nThis is fucking awesome\\n\\nNow\\nWalk into the club like, \"What up? I got a big cock\"\\nNah, I\\'m just pumped, I bought some shit from a thrift shop\\nIce on the fringe is so damn frosty\\nThe people like, \"Damn, that\\'s a cold ass honkey\"\\nRollin\\' in hella deep, headed to the mezzanine\\nDressed in '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data['lyrics'][song_idx]\n",
    "sample[:len(sample)//5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lyric String Passover 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean New-line breaks, but preserve periods\n",
    "data['lyrics'] = data['lyrics'].apply(lambda x: x.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Hey, Macklemore, can we go thrift shopping?\"',\n",
       " 'What what, what, what',\n",
       " 'What what, what, what',\n",
       " 'What what, what, what',\n",
       " 'What what, what, what',\n",
       " 'What what, what, what',\n",
       " 'Bada, bada, bada doo da',\n",
       " 'What what, what, what',\n",
       " 'Bada, bada, bada doo da',\n",
       " 'What what, what, what',\n",
       " 'Bada, bada, bada doo da',\n",
       " 'Bada, bada, bada doo da',\n",
       " 'Bada, bada, bada doo da',\n",
       " 'Bada, bada, bada doo da',\n",
       " 'Bada, bada, bada doo da',\n",
       " '',\n",
       " \"I'm gonna pop some tags\",\n",
       " 'Only got 20 dollars in my pocket',\n",
       " \"I'm, I'm, I'm huntin', lookin' for a come up\",\n",
       " 'This is fucking awesome']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data['lyrics'][song_idx]\n",
    "# Need to join due to splitting to list\n",
    "# \" \".join(sample[:len(sample)//5])\n",
    "sample[:len(sample)//5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lyric String Passover 3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Custom Text Cleaning Function\n",
    "data = clean_text(data, 'lyrics', 'lyrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey macklemore can we go thrift shopping',\n",
       " 'what what what what',\n",
       " 'what what what what',\n",
       " 'what what what what',\n",
       " 'what what what what',\n",
       " 'what what what what',\n",
       " 'bada bada bada doo da',\n",
       " 'what what what what',\n",
       " 'bada bada bada doo da',\n",
       " 'what what what what',\n",
       " 'bada bada bada doo da',\n",
       " 'bada bada bada doo da',\n",
       " 'bada bada bada doo da',\n",
       " 'bada bada bada doo da',\n",
       " 'bada bada bada doo da',\n",
       " '',\n",
       " 'im gonna pop some tags',\n",
       " 'only got 20 dollars in my pocket',\n",
       " 'im im im huntin lookin for a come up',\n",
       " 'this is fucking awesome']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data['lyrics'][song_idx]\n",
    "# Need to join due to splitting to list\n",
    "# \" \".join(sample[:len(sample)//5])\n",
    "sample[:len(sample)//5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 'document' Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Document Feature with Lyrics joined into one string (strips, negates whitespace)\n",
    "data['clean_text'] = data['lyrics'].apply(lambda x: \" \".join([i.strip() for i in x if i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey macklemore can we go thrift shopping what what what what what what what what what what what what what what what what what what what what bada bada bada doo da what what what what bada bada bada doo da what what what what bada bada bada doo da bada bada bada doo da bada bada bada doo da bada bada bada doo da bada bada bada doo da im gonna pop some tags only got 20 dollars in my pocket im im im huntin lookin for a come up this is fucking awesome now walk into the club like what up i got a big cock nah im just pumped i bought some shit from a thrift shop ice on the fringe is so damn frosty the people like damn thats a cold ass honkey rollin in hella deep headed to the mezzanine dressed in all pin'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data['clean_text'][song_idx]\n",
    "# Need to join due to splitting to list\n",
    "sample[:len(sample)//5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVE EXPLETIVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['clean_text'].apply(match_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sent Token Feature\n",
    "data['sentences'] = data['clean_text'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey macklemore can we go thrift shopping what what what what what what what what what what what what what what what what what what what what bada bada bada doo da what what what what bada bada bada doo da what what what what bada bada bada doo da bada bada bada doo da bada bada bada doo da bada bada bada doo da bada bada bada doo da im gonna pop some tags only got 20 dollars in my pocket im im im huntin lookin for a come up this is expletive_3 awesome now walk into the club like what up i got a big expletive_7 nah im just pumped i bought some expletive_4 from a thrift shop ice on the fringe is so damn frosty the people like damn thats a cold expletive_6 honkey rollin in hella deep headed to the mezzanine dressed in all pink cept my gator sexpletive_5 those are green draped in a leopard mink girl standin next to me probably shouldve washed this smells like r kelly sheets expletive_12 but expletive_4 it was 99 cents expletive_3 it coppin it washin it bout to go and get some compliments pexpletive_6 up on those moccasins someone else has been walkin in bummy and grungy expletive_3 it man i am stunting and flossin and saving my money and im hella happy thats a bargain expletive_1 ima take your grandpas style ima take your grandpas style no for real ask your grandpa can i have his handmedowns thank you velour jumpsuit and some house slippers dookie brown leather jacket that i found diggin they had a broken keyboard i bought a broken keyboard i bought a skeet blanket then i bought a knee board hello hello my ace man my mellow john wayne aint got nothing on my fringe game hell no i could take some pro wings make em cool sell those the sneaker heads would be like ah he got the velcros im gonna pop some tags only got 20 dollars in my pocket im im im huntin lookin for a come up this is expletive_3 awesome im gonna pop some tags only got 20 dollars in my pocket im im im huntin lookin for a come up this is expletive_3 awesome what you know about rockin a wolf on your noggin what you knowin about wearin a fur fox skin im diggin im diggin im searchin right through that luggage one mans trash thats another mans come up thank your granddad for donatin that plaid button up shirt cause right now im up in hurr stuntin im at the goodwill you can find me in the bins im not im not stuck on searchin in that section mens your grammy your auntie your mama your mammy ill take those flannel zebra jammies second hand and ill rock that motherexpletive_3 the builtin onesie with the socks on the motherexpletive_3 i hit the party and they stop in that motherexpletive_3 they be like oh that gucci thats hella tight im like yo thats 50 dollars for a tshirt limited edition lets do some simple addition 50 dollars for a tshirt thats just some ignorant expletive_1 expletive_4 i call that gettingswindledandpimped expletive_4 i call that getting tricked by business that shirts hella dough and having the same one as six other people in this club is a hella dont peep game come take a look through my telescope tryna get girls from a brand man you hella wont man you hella wont goodwill poppin tags yeah im gonna pop some tags only got 20 dollars in my pocket im im im huntin lookin for a come up this is expletive_3 awesome ill wear your granddads clothes i look incredible im in this bigexpletive_6 coat from that thrift shop down the road ill wear your granddads clothes damn right i look incredible now come on man im in this bigexpletive_6 coat bigexpletive_6 coat from that thrift shop down the road lets go come on im gonna pop some tags only got 20 dollars in my pocket im im im huntin lookin for a come up this is expletive_3 awesome is that your grandmas coat']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Sentences (truncated)\n",
    "data['sentences'][song_idx][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'macklemore', 'can', 'we', 'go', 'thrift', 'shopping', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what']\n"
     ]
    }
   ],
   "source": [
    "# Create tokens for each song\n",
    "data['tokens'] = data['clean_text'].apply(word_tokenize)\n",
    "print(data['tokens'][song_idx][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtration (Stop-words, punctiation, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'macklemore', 'can', 'we', 'go', 'thrift', 'shopping', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what']\n"
     ]
    }
   ],
   "source": [
    "# Filter Punctuation\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [i for i in x if i not in string.punctuation])\n",
    "print(data['tokens'][song_idx][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'macklemore', 'go', 'thrift', 'shopping', 'bada', 'bada', 'bada', 'doo', 'da', 'bada', 'bada', 'bada', 'doo', 'da', 'bada', 'bada', 'bada', 'doo', 'da']\n"
     ]
    }
   ],
   "source": [
    "# Filter Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words = re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", \" \".join(stop_words)).split() + ['im', 'ill']\n",
    "data['tokens_stop'] = data['tokens'].apply(lambda x: [i for i in x if i not in stop_words])\n",
    "print(data['tokens_stop'][song_idx][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Tokens to Lyric Unique Words (from whole document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n",
      "['hey', 'macklemore', 'can', 'we', 'go', 'thrift', 'shopping', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what']\n",
      "Set (from non tokens): \n",
      "['oh', 'dookie', 'a', 'road', 'expletive_7', 'zebra', 'but', 'about', 'big', 'wolf', 'gucci', 'yeah', 'been', 'next', 'other', 'tags', 'pink', 'thrift', 'leather', 'call']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens: \")\n",
    "print(data['tokens'][song_idx][:20])\n",
    "print(\"Set (from non tokens): \")\n",
    "print(list(set(data['clean_text'][song_idx].split()))[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem or Lemmatize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Porter, Snowball, WordNet Objects\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "# Get functions from each object\n",
    "porter_func = porter.stem\n",
    "snowball_func = snowball.stem\n",
    "wordnet_func = wordnet.lemmatize\n",
    "\n",
    "# Create lambda func to easily apply func to each token\n",
    "get_root = lambda tokens, func: [func(token) for token in tokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Tokens for each type of processor\n",
    "porter_tokens = data['tokens'].apply(lambda x: get_root(x, porter_func)) \n",
    "snowball_tokens = data['tokens'].apply(lambda x: get_root(x, snowball_func)) \n",
    "wordnet_tokens = data['tokens'].apply(lambda x: get_root(x, wordnet_func)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            WORD |           PORTER |         SNOWBALL |       LEMMATIZER |\n",
      "      macklemore |        macklemor |        macklemor |       macklemore |\n",
      "        shopping |             shop |             shop |         shopping |\n",
      "            only |             onli |             onli |             only |\n",
      "            this |              thi |             this |             this |\n",
      "         awesome |           awesom |           awesom |          awesome |\n",
      "          pumped |             pump |             pump |           pumped |\n",
      "          fringe |            fring |            fring |           fringe |\n",
      "          frosty |           frosti |           frosti |           frosty |\n",
      "          people |            peopl |            peopl |           people |\n",
      "           thats |             that |             that |            thats |\n",
      "          headed |             head |             head |           headed |\n",
      "       mezzanine |         mezzanin |         mezzanin |        mezzanine |\n",
      "         dressed |            dress |            dress |          dressed |\n",
      "          draped |            drape |            drape |           draped |\n",
      "        probably |          probabl |          probabl |         probably |\n",
      "        shouldve |          shouldv |          shouldv |         shouldve |\n",
      "          washed |             wash |             wash |           washed |\n",
      "            this |              thi |             this |             this |\n",
      "           kelly |            kelli |            kelli |            kelly |\n",
      "             was |               wa |              was |               wa |\n",
      "         someone |           someon |           someon |          someone |\n",
      "            else |              els |              els |             else |\n",
      "             has |               ha |              has |               ha |\n",
      "           bummy |            bummi |            bummi |            bummy |\n",
      "          grungy |           grungi |           grungi |           grungy |\n",
      "        stunting |            stunt |            stunt |         stunting |\n",
      "          saving |             save |             save |           saving |\n",
      "           happy |            happi |            happi |            happy |\n",
      "           thats |             that |             that |            thats |\n",
      "             his |               hi |              his |              his |\n",
      "     handmedowns |       handmedown |       handmedown |      handmedowns |\n",
      "           house |             hous |             hous |            house |\n",
      "          dookie |            dooki |            dooki |           dookie |\n",
      "           wayne |             wayn |             wayn |            wayne |\n",
      "         nothing |             noth |             noth |          nothing |\n",
      "          fringe |            fring |            fring |           fringe |\n",
      "            only |             onli |             onli |             only |\n",
      "            this |              thi |             this |             this |\n",
      "         awesome |           awesom |           awesom |          awesome |\n",
      "            only |             onli |             onli |             only |\n",
      "            this |              thi |             this |             this |\n",
      "         awesome |           awesom |           awesom |          awesome |\n",
      "         luggage |           luggag |           luggag |          luggage |\n",
      "           thats |             that |             that |            thats |\n",
      "         another |            anoth |            anoth |          another |\n",
      "           cause |             caus |             caus |            cause |\n",
      "        goodwill |          goodwil |          goodwil |         goodwill |\n",
      "          grammy |           grammi |           grammi |           grammy |\n",
      "          auntie |            aunti |            aunti |           auntie |\n",
      "           mammy |            mammi |            mammi |            mammy |\n",
      "         jammies |            jammi |            jammi |          jammies |\n",
      "          onesie |            onesi |            onesi |           onesie |\n",
      "           party |            parti |            parti |            party |\n",
      "           thats |             that |             that |            thats |\n",
      "           thats |             that |             that |            thats |\n",
      "         limited |            limit |            limit |          limited |\n",
      "         edition |             edit |             edit |          edition |\n",
      "          simple |            simpl |            simpl |           simple |\n",
      "        addition |            addit |            addit |         addition |\n",
      "           thats |             that |             that |            thats |\n",
      "        ignorant |            ignor |            ignor |         ignorant |\n",
      "gettingswindledandpimped | gettingswindledandpimp | gettingswindledandpimp | gettingswindledandpimped |\n",
      "         getting |              get |              get |          getting |\n",
      "         tricked |            trick |            trick |          tricked |\n",
      "        business |             busi |             busi |         business |\n",
      "          having |             have |             have |           having |\n",
      "              as |               as |               as |                a |\n",
      "          people |            peopl |            peopl |           people |\n",
      "            this |              thi |             this |             this |\n",
      "       telescope |         telescop |         telescop |        telescope |\n",
      "        goodwill |          goodwil |          goodwil |         goodwill |\n",
      "            only |             onli |             onli |             only |\n",
      "            this |              thi |             this |             this |\n",
      "         awesome |           awesom |           awesom |          awesome |\n",
      "         clothes |            cloth |            cloth |          clothes |\n",
      "      incredible |           incred |           incred |       incredible |\n",
      "            this |              thi |             this |             this |\n",
      "         clothes |            cloth |            cloth |          clothes |\n",
      "      incredible |           incred |           incred |       incredible |\n",
      "            this |              thi |             this |             this |\n",
      "            only |             onli |             onli |             only |\n",
      "            this |              thi |             this |             this |\n",
      "         awesome |           awesom |           awesom |          awesome |\n"
     ]
    }
   ],
   "source": [
    "## Print the stemmed and lemmatized words from the target document\n",
    "print(\"%16s | %16s | %16s | %16s |\" % (\"WORD\", \"PORTER\", \"SNOWBALL\", \"LEMMATIZER\"))\n",
    "for i in range(min(len(porter_tokens[song_idx]), len(snowball_tokens[song_idx]), len(wordnet_tokens[song_idx]))):\n",
    "    p, s, w = porter_tokens[song_idx][i], snowball_tokens[song_idx][i], wordnet_tokens[song_idx][i]\n",
    "    if len(set((p, s, w))) != 1:\n",
    "        print(\"%16s | %16s | %16s | %16s |\" % (data['tokens'][song_idx][i], p, s, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, the results show that using any type of stemmer or lemmatizer seemed to detract from the words rather than help center them. These methods of word procession are not able to account for the colloqualisms that come from the language of rap. Therefore we will not proceed with using this for any word processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Features Added\n",
    "\n",
    "Now that the tokens are extracted from the lyric set, it's time to create a new feature with the SET of tokens for ease of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh',\n",
       " 'dookie',\n",
       " 'a',\n",
       " 'road',\n",
       " 'expletive_7',\n",
       " 'zebra',\n",
       " 'but',\n",
       " 'about',\n",
       " 'big',\n",
       " 'wolf']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Token Set Feature\n",
    "data['token_set'] = data['tokens'].apply(lambda x: list(set(x)))\n",
    "data['token_set'][song_idx][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams\n",
    "\n",
    "It might be useful to see if N-Grams would give us a better list of tokens, since most rap lyrics involve heavy use of consecutive and connected words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hey', 'macklemore'),\n",
       " ('macklemore', 'can'),\n",
       " ('can', 'we'),\n",
       " ('we', 'go'),\n",
       " ('go', 'thrift'),\n",
       " ('thrift', 'shopping'),\n",
       " ('shopping', 'what'),\n",
       " ('what', 'what'),\n",
       " ('what', 'what'),\n",
       " ('what', 'what')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(data['tokens'][song_idx], 2))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['oh', 'UH'],\n",
       "       ['dookie', 'NN'],\n",
       "       ['a', 'DT'],\n",
       "       ['road', 'NN'],\n",
       "       ['expletive_7', 'NN'],\n",
       "       ['zebra', 'NN'],\n",
       "       ['but', 'CC'],\n",
       "       ['about', 'IN'],\n",
       "       ['big', 'JJ'],\n",
       "       ['wolf', 'NN']], dtype='<U11')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimalize Tag Functions\n",
    "pos_tagger = nltk.pos_tag\n",
    "explain_tag = nltk.help.upenn_tagset\n",
    "\n",
    "# Get Sample Tags\n",
    "tag_sample = np.array(pos_tagger(set(data['tokens'][song_idx]))[:10])\n",
    "tag_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "WORDS: ['but']\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "WORDS: ['a']\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "WORDS: ['about']\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "WORDS: ['big']\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "WORDS: ['dookie', 'road', 'expletive_7', 'zebra', 'wolf']\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "WORDS: ['oh']\n"
     ]
    }
   ],
   "source": [
    "# Split Words and Tags\n",
    "words, tags = tag_sample[:, 0], tag_sample[:, 1]\n",
    "\n",
    "# Create DF to Groupby\n",
    "tag_df = pd.DataFrame({'words':words, 'tags':tags})\n",
    "grouped_tags = tag_df.groupby('tags')\n",
    "\n",
    "for x in grouped_tags:\n",
    "    word = x[1]['words']\n",
    "    explain_tag(x[0])\n",
    "    print(f'WORDS: {word.tolist()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using POS Tagging seems to give more insight to the words and what they represent, but similarly to the Stemmers/Lemmatizers, they seem to also miscategorize things. The word 'patek' is actually a brand reference to Patek Watches, a luxury watch brand, and is not a verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Extra Column\n",
    "data = data.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, export data to clean_data.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_pickle('data/clean_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('data/all_clean_data.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
