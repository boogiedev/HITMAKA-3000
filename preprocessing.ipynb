{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto Reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Modules\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from pprint import pprint\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from math import log\n",
    "import re\n",
    "\n",
    "\n",
    "# NLTK Modules\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('tagsets')\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import chunk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Import Custom Modules\n",
    "from src.data_cleaner import *\n",
    "from src.dummy_words import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Scraping Program\n",
    "# !python src/web_scraper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intake Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>featured</th>\n",
       "      <th>rank</th>\n",
       "      <th>year</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>lyrics_state</th>\n",
       "      <th>song_id</th>\n",
       "      <th>lyrics_owner_id</th>\n",
       "      <th>primary_artist_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>See You Again</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>It's been a long day without you, my friend\\nA...</td>\n",
       "      <td>True</td>\n",
       "      <td>720401</td>\n",
       "      <td>341761</td>\n",
       "      <td>https://genius.com/artists/Wiz-khalifa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Trap Queen</td>\n",
       "      <td>Fetty Wap</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>RGF productions\\nRemy Boyz, yah-ah\\n1738, ayy\\...</td>\n",
       "      <td>True</td>\n",
       "      <td>496445</td>\n",
       "      <td>104344</td>\n",
       "      <td>https://genius.com/artists/Fetty-wap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Watch Me</td>\n",
       "      <td>Silento</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "      <td>Whip, nae nae\\nWhip, whip, nae nae\\nWhip, nae ...</td>\n",
       "      <td>True</td>\n",
       "      <td>1743010</td>\n",
       "      <td>1696010</td>\n",
       "      <td>https://genius.com/artists/Silento</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           song        artist  featured  rank  year  \\\n",
       "0           0  See You Again  Wiz Khalifa        NaN     1  2015   \n",
       "1           1     Trap Queen     Fetty Wap       NaN     2  2015   \n",
       "2           2       Watch Me       Silento       NaN     3  2015   \n",
       "\n",
       "                                              lyrics  lyrics_state  song_id  \\\n",
       "0  It's been a long day without you, my friend\\nA...          True   720401   \n",
       "1  RGF productions\\nRemy Boyz, yah-ah\\n1738, ayy\\...          True   496445   \n",
       "2  Whip, nae nae\\nWhip, whip, nae nae\\nWhip, nae ...          True  1743010   \n",
       "\n",
       "   lyrics_owner_id                      primary_artist_url  \n",
       "0           341761  https://genius.com/artists/Wiz-khalifa  \n",
       "1           104344    https://genius.com/artists/Fetty-wap  \n",
       "2          1696010      https://genius.com/artists/Silento  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Data\n",
    "data = pd.read_csv('data/data.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                            0\n",
       "song                                                      See You Again\n",
       "artist                                                     Wiz Khalifa \n",
       "featured                                                            NaN\n",
       "rank                                                                  1\n",
       "year                                                               2015\n",
       "lyrics                It's been a long day without you, my friend\\nA...\n",
       "lyrics_state                                                       True\n",
       "song_id                                                          720401\n",
       "lyrics_owner_id                                                  341761\n",
       "primary_artist_url               https://genius.com/artists/Wiz-khalifa\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global Variables\n",
    "song_idx = 0\n",
    "data.iloc[song_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating pipeline for **indexing** song lyrics (document).\n",
    "\n",
    "This will lead to **indexing** which creates a **signature** (vector) for each document.\n",
    "\n",
    "Then, the **signatures** will be used for relating documents one to the other (and find out similar clusters of documents), or for mining underlying relations between concepts.\n",
    "\n",
    "<img src=\"media/text-pipeline.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bags of Words: Lyric String Example\n",
    "> With the intake lyric data, it seems that there needs to be a couple things cleaned. Casing, punctuation, and new-lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lyric String Passover 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's been a long day without you, my friend\\nAnd I'll tell you all about it when I see you again\\nWe've come a long way from where we began\\nOh, I'll tell you all about it when I see you again\\nWhen I see you again\\n\\nDamn, who knew?\\nAll the planes we flew, good things we been through\\nThat I'd be standing right here talking to you\\n'Bout another path, I know we loved to hit the road and laugh\\nBut something told me that it wouldn't last\\nHad to switch\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data['lyrics'][song_idx]\n",
    "sample[:len(sample)//5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lyric String Passover 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean New-line breaks, but preserve periods\n",
    "data['lyrics'] = data['lyrics'].apply(lambda x: x.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's been a long day without you, my friend\",\n",
       " \"And I'll tell you all about it when I see you again\",\n",
       " \"We've come a long way from where we began\",\n",
       " \"Oh, I'll tell you all about it when I see you again\",\n",
       " 'When I see you again',\n",
       " '',\n",
       " 'Damn, who knew?',\n",
       " 'All the planes we flew, good things we been through',\n",
       " \"That I'd be standing right here talking to you\",\n",
       " \"'Bout another path, I know we loved to hit the road and laugh\",\n",
       " \"But something told me that it wouldn't last\",\n",
       " 'Had to switch up, look at things different, see the bigger picture']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data['lyrics'][song_idx]\n",
    "# Need to join due to splitting to list\n",
    "# \" \".join(sample[:len(sample)//5])\n",
    "sample[:len(sample)//5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lyric String Passover 3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Custom Text Cleaning Function\n",
    "data = clean_text(data, 'lyrics', 'lyrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['its been a long day without you my friend',\n",
       " 'and ill tell you all about it when i see you again',\n",
       " 'weve come a long way from where we began',\n",
       " 'oh ill tell you all about it when i see you again',\n",
       " 'when i see you again',\n",
       " '',\n",
       " 'damn who knew',\n",
       " 'all the planes we flew good things we been through',\n",
       " 'that id be standing right here talking to you',\n",
       " 'bout another path i know we loved to hit the road and laugh',\n",
       " 'but something told me that it wouldnt last',\n",
       " 'had to switch up look at things different see the bigger picture']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data['lyrics'][song_idx]\n",
    "# Need to join due to splitting to list\n",
    "# \" \".join(sample[:len(sample)//5])\n",
    "sample[:len(sample)//5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 'document' Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Document Feature with Lyrics joined into one string (strips, negates whitespace)\n",
    "data['clean_text'] = data['lyrics'].apply(lambda x: \" \".join([i.strip() for i in x if i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'its been a long day without you my friend and ill tell you all about it when i see you again weve come a long way from where we began oh ill tell you all about it when i see you again when i see you again damn who knew all the planes we flew good things we been through that id be standing right here talking to you bout another path i know we loved to hit the road and laugh but something told me that it wouldnt last had to '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data['clean_text'][song_idx]\n",
    "# Need to join due to splitting to list\n",
    "sample[:len(sample)//5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVE EXPLETIVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['clean_text'].apply(match_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sent Token Feature\n",
    "data['sentences'] = data['clean_text'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['its been a long day without you my friend and ill tell you all about it when i see you again weve come a long way from where we began oh ill tell you all about it when i see you again when i see you again damn who knew all the planes we flew good things we been through that id be standing right here talking to you bout another path i know we loved to hit the road and laugh but something told me that it wouldnt last had to switch up look at things different see the bigger picture those were the days hard work forever pays now i see you in a better place see you in a better place uh how could we not talk about family when familys all that we got everything i went through you were standing there by my side and now you gon be with me for the last ride its been a long day without you my friend and ill tell you all about it when i see you again ill see you again weve come a long way yeah we came a long way from where we began you know we started oh ill tell you all about it when i see you again ill tell you when i see you again aah oh aah oh woooohohohohohoh yeah first you both go out your way and the vibe is feeling strong and whats small turned to a friendship a friendship turned to a bond and that bond will never be broken the love will never get lost the love will never get lost and when brotherhood come first then the line will never be crossed established it on our own when that line had to be drawn and that line is what we reached so remember me when im gone remember me when im gone how could we not talk about family when familys all that we got everything i went through you were standing there by my side and now you gon be with me for the last ride so let the light guide your way yeah hold every memory as you go and every road you take will always lead you home home its been a long day without you my friend and ill tell you all about it when i see you again weve come a long way from where we began oh ill tell you all about it when i see you again when i see you again aah oh aah oh uh yeah yeah woooohohohohohoh yo when i see you again yo uh see you again yo yo ohoh uhhuh yup when i see you again']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Sentences (truncated)\n",
    "data['sentences'][song_idx][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['its', 'been', 'a', 'long', 'day', 'without', 'you', 'my', 'friend', 'and', 'ill', 'tell', 'you', 'all', 'about', 'it', 'when', 'i', 'see', 'you']\n"
     ]
    }
   ],
   "source": [
    "# Create tokens for each song\n",
    "data['tokens'] = data['clean_text'].apply(word_tokenize)\n",
    "print(data['tokens'][song_idx][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtration (Stop-words, punctiation, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['its', 'been', 'a', 'long', 'day', 'without', 'you', 'my', 'friend', 'and', 'ill', 'tell', 'you', 'all', 'about', 'it', 'when', 'i', 'see', 'you']\n"
     ]
    }
   ],
   "source": [
    "# Filter Punctuation\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [i for i in x if i not in string.punctuation])\n",
    "print(data['tokens'][song_idx][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['long', 'day', 'without', 'friend', 'tell', 'see', 'weve', 'come', 'long', 'way', 'began', 'oh', 'tell', 'see', 'see', 'damn', 'knew', 'planes', 'flew', 'good']\n"
     ]
    }
   ],
   "source": [
    "# Filter Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words = re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", \" \".join(stop_words)).split() + ['im', 'ill']\n",
    "data['tokens_stop'] = data['tokens'].apply(lambda x: [i for i in x if i not in stop_words])\n",
    "print(data['tokens_stop'][song_idx][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Tokens to Lyric Unique Words (from whole document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n",
      "['its', 'been', 'a', 'long', 'day', 'without', 'you', 'my', 'friend', 'and', 'ill', 'tell', 'you', 'all', 'about', 'it', 'when', 'i', 'see', 'you']\n",
      "Set (from non tokens): \n",
      "['about', 'day', 'bigger', 'me', 'uh', 'how', 'came', 'place', 'ride', 'strong', 'get', 'is', 'friend', 'drawn', 'switch', 'always', 'take', 'picture', 'got', 'path']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens: \")\n",
    "print(data['tokens'][song_idx][:20])\n",
    "print(\"Set (from non tokens): \")\n",
    "print(list(set(data['clean_text'][song_idx].split()))[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem or Lemmatize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Porter, Snowball, WordNet Objects\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "# Get functions from each object\n",
    "porter_func = porter.stem\n",
    "snowball_func = snowball.stem\n",
    "wordnet_func = wordnet.lemmatize\n",
    "\n",
    "# Create lambda func to easily apply func to each token\n",
    "get_root = lambda tokens, func: [func(token) for token in tokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Tokens for each type of processor\n",
    "porter_tokens = data['tokens'].apply(lambda x: get_root(x, porter_func)) \n",
    "snowball_tokens = data['tokens'].apply(lambda x: get_root(x, snowball_func)) \n",
    "wordnet_tokens = data['tokens'].apply(lambda x: get_root(x, wordnet_func)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            WORD |           PORTER |         SNOWBALL |       LEMMATIZER |\n",
      "        standing |            stand |            stand |         standing |\n",
      "         talking |             talk |             talk |          talking |\n",
      "         another |            anoth |            anoth |          another |\n",
      "           loved |             love |             love |            loved |\n",
      "       something |           someth |           someth |        something |\n",
      "       different |           differ |           differ |        different |\n",
      "         picture |           pictur |           pictur |          picture |\n",
      "         forever |            forev |            forev |          forever |\n",
      "          family |           famili |           famili |           family |\n",
      "         familys |           famili |           famili |           family |\n",
      "      everything |          everyth |          everyth |       everything |\n",
      "        standing |            stand |            stand |         standing |\n",
      "         started |            start |            start |          started |\n",
      "         feeling |             feel |             feel |          feeling |\n",
      "           whats |             what |             what |            whats |\n",
      "          turned |             turn |             turn |           turned |\n",
      "          turned |             turn |             turn |           turned |\n",
      "         crossed |            cross |            cross |          crossed |\n",
      "     established |        establish |        establish |      established |\n",
      "         reached |            reach |            reach |          reached |\n",
      "        remember |           rememb |           rememb |         remember |\n",
      "        remember |           rememb |           rememb |         remember |\n",
      "          family |           famili |           famili |           family |\n",
      "         familys |           famili |           famili |           family |\n",
      "      everything |          everyth |          everyth |       everything |\n",
      "        standing |            stand |            stand |         standing |\n",
      "           guide |             guid |             guid |            guide |\n",
      "           every |            everi |            everi |            every |\n",
      "          memory |           memori |           memori |           memory |\n",
      "              as |               as |               as |                a |\n",
      "           every |            everi |            everi |            every |\n",
      "          always |            alway |            alway |           always |\n"
     ]
    }
   ],
   "source": [
    "## Print the stemmed and lemmatized words from the target document\n",
    "print(\"%16s | %16s | %16s | %16s |\" % (\"WORD\", \"PORTER\", \"SNOWBALL\", \"LEMMATIZER\"))\n",
    "for i in range(min(len(porter_tokens[song_idx]), len(snowball_tokens[song_idx]), len(wordnet_tokens[song_idx]))):\n",
    "    p, s, w = porter_tokens[song_idx][i], snowball_tokens[song_idx][i], wordnet_tokens[song_idx][i]\n",
    "    if len(set((p, s, w))) != 1:\n",
    "        print(\"%16s | %16s | %16s | %16s |\" % (data['tokens'][song_idx][i], p, s, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, the results show that using any type of stemmer or lemmatizer seemed to detract from the words rather than help center them. These methods of word procession are not able to account for the colloqualisms that come from the language of rap. Therefore we will not proceed with using this for any word processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Features Added\n",
    "\n",
    "Now that the tokens are extracted from the lyric set, it's time to create a new feature with the SET of tokens for ease of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about',\n",
       " 'day',\n",
       " 'bigger',\n",
       " 'me',\n",
       " 'uh',\n",
       " 'how',\n",
       " 'came',\n",
       " 'place',\n",
       " 'ride',\n",
       " 'strong']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Token Set Feature\n",
    "data['token_set'] = data['tokens'].apply(lambda x: list(set(x)))\n",
    "data['token_set'][song_idx][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams\n",
    "\n",
    "It might be useful to see if N-Grams would give us a better list of tokens, since most rap lyrics involve heavy use of consecutive and connected words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('its', 'been'),\n",
       " ('been', 'a'),\n",
       " ('a', 'long'),\n",
       " ('long', 'day'),\n",
       " ('day', 'without'),\n",
       " ('without', 'you'),\n",
       " ('you', 'my'),\n",
       " ('my', 'friend'),\n",
       " ('friend', 'and'),\n",
       " ('and', 'ill')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(data['tokens'][song_idx], 2))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['about', 'RB'],\n",
       "       ['day', 'NN'],\n",
       "       ['bigger', 'JJR'],\n",
       "       ['me', 'PRP'],\n",
       "       ['uh', 'VB'],\n",
       "       ['how', 'WRB'],\n",
       "       ['came', 'VBD'],\n",
       "       ['place', 'NN'],\n",
       "       ['ride', 'NN'],\n",
       "       ['strong', 'JJ']], dtype='<U6')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimalize Tag Functions\n",
    "pos_tagger = nltk.pos_tag\n",
    "explain_tag = nltk.help.upenn_tagset\n",
    "\n",
    "# Get Sample Tags\n",
    "tag_sample = np.array(pos_tagger(set(data['tokens'][song_idx]))[:10])\n",
    "tag_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "WORDS: ['strong']\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "WORDS: ['bigger']\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "WORDS: ['day', 'place', 'ride']\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "WORDS: ['me']\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "WORDS: ['about']\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "WORDS: ['uh']\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "WORDS: ['came']\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "WORDS: ['how']\n"
     ]
    }
   ],
   "source": [
    "# Split Words and Tags\n",
    "words, tags = tag_sample[:, 0], tag_sample[:, 1]\n",
    "\n",
    "# Create DF to Groupby\n",
    "tag_df = pd.DataFrame({'words':words, 'tags':tags})\n",
    "grouped_tags = tag_df.groupby('tags')\n",
    "\n",
    "for x in grouped_tags:\n",
    "    word = x[1]['words']\n",
    "    explain_tag(x[0])\n",
    "    print(f'WORDS: {word.tolist()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using POS Tagging seems to give more insight to the words and what they represent, but similarly to the Stemmers/Lemmatizers, they seem to also miscategorize things. The word 'patek' is actually a brand reference to Patek Watches, a luxury watch brand, and is not a verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Extra Column\n",
    "data = data.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, export data to clean_data.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('data/clean_data.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
