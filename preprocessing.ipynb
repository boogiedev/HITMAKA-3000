{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto Reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Modules\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from pprint import pprint\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from math import log\n",
    "import re\n",
    "\n",
    "\n",
    "# NLTK Modules\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import Custom Modules\n",
    "from src.data_cleaner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Scraping Program\n",
    "# !python src/web_scraper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intake Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>featured</th>\n",
       "      <th>rank</th>\n",
       "      <th>year</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>lyrics_state</th>\n",
       "      <th>song_id</th>\n",
       "      <th>lyrics_owner_id</th>\n",
       "      <th>primary_artist_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>See You Again</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>It's been a long day without you, my friend\\nA...</td>\n",
       "      <td>True</td>\n",
       "      <td>720401</td>\n",
       "      <td>341761</td>\n",
       "      <td>https://genius.com/artists/Wiz-khalifa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Trap Queen</td>\n",
       "      <td>Fetty Wap</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>RGF productions\\nRemy Boyz, yah-ah\\n1738, ayy\\...</td>\n",
       "      <td>True</td>\n",
       "      <td>496445</td>\n",
       "      <td>104344</td>\n",
       "      <td>https://genius.com/artists/Fetty-wap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Watch Me</td>\n",
       "      <td>Silento</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "      <td>Whip, nae nae\\nWhip, whip, nae nae\\nWhip, nae ...</td>\n",
       "      <td>True</td>\n",
       "      <td>1743010</td>\n",
       "      <td>1696010</td>\n",
       "      <td>https://genius.com/artists/Silento</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           song        artist  featured  rank  year  \\\n",
       "0           0  See You Again  Wiz Khalifa        NaN     1  2015   \n",
       "1           1     Trap Queen     Fetty Wap       NaN     2  2015   \n",
       "2           2       Watch Me       Silento       NaN     3  2015   \n",
       "\n",
       "                                              lyrics  lyrics_state  song_id  \\\n",
       "0  It's been a long day without you, my friend\\nA...          True   720401   \n",
       "1  RGF productions\\nRemy Boyz, yah-ah\\n1738, ayy\\...          True   496445   \n",
       "2  Whip, nae nae\\nWhip, whip, nae nae\\nWhip, nae ...          True  1743010   \n",
       "\n",
       "   lyrics_owner_id                      primary_artist_url  \n",
       "0           341761  https://genius.com/artists/Wiz-khalifa  \n",
       "1           104344    https://genius.com/artists/Fetty-wap  \n",
       "2          1696010      https://genius.com/artists/Silento  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Data\n",
    "data = pd.read_csv('data/data.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>featured</th>\n",
       "      <th>rank</th>\n",
       "      <th>year</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>lyrics_state</th>\n",
       "      <th>song_id</th>\n",
       "      <th>lyrics_owner_id</th>\n",
       "      <th>primary_artist_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>116</td>\n",
       "      <td>Rake It Up</td>\n",
       "      <td>Yo Gotti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>2017</td>\n",
       "      <td>Ear Drummers\\n30, you a fool for this one\\nAh,...</td>\n",
       "      <td>True</td>\n",
       "      <td>3105241</td>\n",
       "      <td>250794</td>\n",
       "      <td>https://genius.com/artists/Yo-gotti</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0        song     artist  featured  rank  year  \\\n",
       "116         116  Rake It Up  Yo Gotti        NaN    17  2017   \n",
       "\n",
       "                                                lyrics  lyrics_state  song_id  \\\n",
       "116  Ear Drummers\\n30, you a fool for this one\\nAh,...          True  3105241   \n",
       "\n",
       "     lyrics_owner_id                   primary_artist_url  \n",
       "116           250794  https://genius.com/artists/Yo-gotti  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global Variables\n",
    "song_idx = 116\n",
    "data[data['song'] == \"Rake It Up\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyric String Example\n",
    "> With the intake lyric data, it seems that there needs to be a couple things cleaned. Casing, punctuation, and new-lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lyric String Passover 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ear Drummers\\n30, you a fool for this one\\nAh, this the strip club anthem, nigga, what\\'s up?\\nYoung Money!\\nYeah, me and Mike WiLL pull up to AOD back to back\\nThem AMG 63\\'s\\nMike WiLL Made-It, nigga\\n\\nI tell all my hoes, \"Rake it up\\nBreak it down, bag it up\"\\nFuck it up, fuck it up (fuck it up, fuck it up)\\nBack it up, back it up (back it up, back it up)\\nRake it up, rake it up (rake it up, rake it up)\\nBack it up, back it up (back it up, back it up)\\nI tell all my hoes (what?) \"Rake it up\\nBreak it down, bag it up\" (bag it up, bag it up)\\nFuck it up, fuck it up (fuck it up)\\nFuck it up, fuck it up (fuck it up)\\nFuck it up, fuck it up (fuck it up)\\nRake it up, rake it up (rake it up)\\n\\nI made love to a stripper (stripper), first I had to tip her (phrrr)\\nTwenty thousand ones (woo), she said I\\'m that nigga (I am)\\nI said, \"I\\'m that nigga, bitch, I already know it\" (I know it)\\nI come with bad weather (ksh), they say I\\'m a storm (ayy)\\nVVS\\'s in my cha'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data['lyrics'][song_idx]\n",
    "sample[:len(sample)//5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lyric String Passover 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean New-line breaks, but preserve periods\n",
    "data['lyrics'] = data['lyrics'].apply(lambda x: x.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ear Drummers',\n",
       " '30, you a fool for this one',\n",
       " \"Ah, this the strip club anthem, nigga, what's up?\",\n",
       " 'Young Money!',\n",
       " 'Yeah, me and Mike WiLL pull up to AOD back to back',\n",
       " \"Them AMG 63's\",\n",
       " 'Mike WiLL Made-It, nigga',\n",
       " '',\n",
       " 'I tell all my hoes, \"Rake it up',\n",
       " 'Break it down, bag it up\"',\n",
       " 'Fuck it up, fuck it up (fuck it up, fuck it up)',\n",
       " 'Back it up, back it up (back it up, back it up)',\n",
       " 'Rake it up, rake it up (rake it up, rake it up)',\n",
       " 'Back it up, back it up (back it up, back it up)',\n",
       " 'I tell all my hoes (what?) \"Rake it up',\n",
       " 'Break it down, bag it up\" (bag it up, bag it up)',\n",
       " 'Fuck it up, fuck it up (fuck it up)',\n",
       " 'Fuck it up, fuck it up (fuck it up)',\n",
       " 'Fuck it up, fuck it up (fuck it up)',\n",
       " 'Rake it up, rake it up (rake it up)',\n",
       " '',\n",
       " 'I made love to a stripper (stripper), first I had to tip her (phrrr)']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data['lyrics'][song_idx]\n",
    "# Need to join due to splitting to list\n",
    "# \" \".join(sample[:len(sample)//5])\n",
    "sample[:len(sample)//5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lyric String Passover 3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Custom Text Cleaning Function\n",
    "data = clean_text(data, 'lyrics', 'lyrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ear drummers',\n",
       " '30 you a fool for this one',\n",
       " 'ah this the strip club anthem nigga whats up',\n",
       " 'young money',\n",
       " 'yeah me and mike will pull up to aod back to back',\n",
       " 'them amg 63s',\n",
       " 'mike will madeit nigga',\n",
       " '',\n",
       " 'i tell all my hoes rake it up',\n",
       " 'break it down bag it up',\n",
       " 'fuck it up fuck it up fuck it up fuck it up',\n",
       " 'back it up back it up back it up back it up',\n",
       " 'rake it up rake it up rake it up rake it up',\n",
       " 'back it up back it up back it up back it up',\n",
       " 'i tell all my hoes what rake it up',\n",
       " 'break it down bag it up bag it up bag it up',\n",
       " 'fuck it up fuck it up fuck it up',\n",
       " 'fuck it up fuck it up fuck it up',\n",
       " 'fuck it up fuck it up fuck it up',\n",
       " 'rake it up rake it up rake it up',\n",
       " '',\n",
       " 'i made love to a stripper stripper first i had to tip her phrrr']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data['lyrics'][song_idx]\n",
    "# Need to join due to splitting to list\n",
    "# \" \".join(sample[:len(sample)//5])\n",
    "sample[:len(sample)//5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 'document' Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Document Feature with Lyrics joined into one string (strips, negates whitespace)\n",
    "data['clean_text'] = data['lyrics'].apply(lambda x: \". \".join([i.strip() for i in x if i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ear drummers. 30 you a fool for this one. ah this the strip club anthem nigga whats up. young money. yeah me and mike will pull up to aod back to back. them amg 63s. mike will madeit nigga. i tell all my hoes rake it up. break it down bag it up. fuck it up fuck it up fuck it up fuck it up. back it up back it up back it up back it up. rake it up rake it up rake it up rake it up. back it up back it up back it up back it up. i tell all my hoes what rake it up. break it down bag it up bag it up bag it up. fuck it up fuck it up fuck it up. fuck it up fuck it up fuck it up. fuck it up fuck it up fuck it up. rake it up rake it up rake it up. i made love to a stripper stripper first i had to tip her phrrr. twenty thousand ones woo she said im that nigga i am. i said im that nigga bitch i already know it i know it. i come with bad weather ksh they say im a storm ayy. vvss in my charm that'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data['clean_text'][song_idx]\n",
    "# Need to join due to splitting to list\n",
    "sample[:len(sample)//5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sent Token Feature\n",
    "data['sentences'] = data['clean_text'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ear drummers.',\n",
       " '30 you a fool for this one.',\n",
       " 'ah this the strip club anthem nigga whats up.',\n",
       " 'young money.',\n",
       " 'yeah me and mike will pull up to aod back to back.',\n",
       " 'them amg 63s.',\n",
       " 'mike will madeit nigga.',\n",
       " 'i tell all my hoes rake it up.',\n",
       " 'break it down bag it up.',\n",
       " 'fuck it up fuck it up fuck it up fuck it up.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Sentences (truncated)\n",
    "data['sentences'][song_idx][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ear', 'drummers', '.', '30', 'you', 'a', 'fool', 'for', 'this', 'one', '.', 'ah', 'this', 'the', 'strip', 'club', 'anthem', 'nigga', 'whats', 'up']\n"
     ]
    }
   ],
   "source": [
    "# Create tokens for each song\n",
    "data['tokens'] = data['clean_text'].apply(word_tokenize)\n",
    "print(data['tokens'][song_idx][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtration (Stop-words, punctiation, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ear', 'drummers', '30', 'you', 'a', 'fool', 'for', 'this', 'one', 'ah', 'this', 'the', 'strip', 'club', 'anthem', 'nigga', 'whats', 'up', 'young', 'money']\n"
     ]
    }
   ],
   "source": [
    "# Filter Punctuation\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [i for i in x if i not in string.punctuation])\n",
    "print(data['tokens'][song_idx][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ear', 'drummers', '30', 'fool', 'one', 'ah', 'strip', 'club', 'anthem', 'nigga', 'whats', 'young', 'money', 'yeah', 'mike', 'pull', 'aod', 'back', 'back', 'amg']\n"
     ]
    }
   ],
   "source": [
    "# Filter Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [i for i in x if i not in stop_words])\n",
    "print(data['tokens'][song_idx][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Tokens to Lyric Unique Words (from whole document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n",
      "['ear', 'drummers', '30', 'fool', 'one', 'ah', 'strip', 'club', 'anthem', 'nigga', 'whats', 'young', 'money', 'yeah', 'mike', 'pull', 'aod', 'back', 'back', 'amg']\n",
      "Set (from non tokens): \n",
      "['punani', 'strip', 'her', 'still', 'throw', 'dough.', 'got', 'club', 'be', 'about.', 'big', 'pussy', 'lil', 'pull', 'drummers.', 'baby', 'social', 'thats', 'ima', 'statement']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens: \")\n",
    "print(data['tokens'][song_idx][:20])\n",
    "print(\"Set (from non tokens): \")\n",
    "print(list(set(data['clean_text'][song_idx].split()))[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem or Lemmatize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Porter, Snowball, WordNet Objects\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "# Get functions from each object\n",
    "porter_func = porter.stem\n",
    "snowball_func = snowball.stem\n",
    "wordnet_func = wordnet.lemmatize\n",
    "\n",
    "# Create lambda func to easily apply func to each token\n",
    "get_root = lambda tokens, func: [func(token) for token in tokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Tokens for each type of processor\n",
    "porter_tokens = data['tokens'].apply(lambda x: get_root(x, porter_func)) \n",
    "snowball_tokens = data['tokens'].apply(lambda x: get_root(x, snowball_func)) \n",
    "wordnet_tokens = data['tokens'].apply(lambda x: get_root(x, wordnet_func)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            WORD |           PORTER |         SNOWBALL |       LEMMATIZER |\n",
      "           whats |             what |             what |            whats |\n",
      "             63s |               63 |              63s |              63s |\n",
      "          twenty |           twenti |           twenti |           twenty |\n",
      "         already |          alreadi |          alreadi |          already |\n",
      "             ayy |              ayi |              ayi |              ayy |\n",
      "           thats |             that |             that |            thats |\n",
      "        phillipe |          phillip |          phillip |         phillipe |\n",
      "          picked |             pick |             pick |           picked |\n",
      "           pussy |            pussi |            pussi |            pussy |\n",
      "           pussy |            pussi |            pussi |            pussy |\n",
      "           pussy |            pussi |            pussi |            pussy |\n",
      "           pussy |            pussi |            pussi |            pussy |\n",
      "           asked |              ask |              ask |            asked |\n",
      "         forgive |           forgiv |           forgiv |          forgive |\n",
      "           cause |             caus |             caus |            cause |\n",
      "          prayed |             pray |             pray |           prayed |\n",
      "           pussy |            pussi |            pussi |            pussy |\n",
      "          prayed |             pray |             pray |           prayed |\n",
      "           pussy |            pussi |            pussi |            pussy |\n",
      "           mixed |              mix |              mix |            mixed |\n",
      "         smuggle |           smuggl |           smuggl |          smuggle |\n",
      "            buss |             buss |             buss |              bus |\n",
      "           goofy |            goofi |            goofi |            goofy |\n",
      "             yes |               ye |              yes |              yes |\n",
      "          cookie |            cooki |            cooki |           cookie |\n",
      "          bonnie |            bonni |            bonni |           bonnie |\n",
      "          ronnie |            ronni |            ronni |           ronnie |\n",
      "         supreme |           suprem |           suprem |          supreme |\n",
      "           bimmy |            bimmi |            bimmi |            bimmy |\n",
      "        nickname |          nicknam |          nicknam |         nickname |\n",
      "          nicole |            nicol |            nicol |           nicole |\n",
      "             ass |              ass |              ass |               as |\n",
      "           cause |             caus |             caus |            cause |\n",
      "           thats |             that |             that |            thats |\n",
      "          expose |            expos |            expos |           expose |\n",
      "           thats |             that |             that |            thats |\n",
      "          little |            littl |            littl |           little |\n",
      "           bitty |            bitti |            bitti |            bitty |\n",
      "          hustle |            hustl |            hustl |           hustle |\n",
      "            baby |             babi |             babi |             baby |\n",
      "              us |               us |               us |                u |\n"
     ]
    }
   ],
   "source": [
    "## Print the stemmed and lemmatized words from the target document\n",
    "print(\"%16s | %16s | %16s | %16s |\" % (\"WORD\", \"PORTER\", \"SNOWBALL\", \"LEMMATIZER\"))\n",
    "for i in range(min(len(porter_tokens[song_idx]), len(snowball_tokens[song_idx]), len(wordnet_tokens[song_idx]))):\n",
    "    p, s, w = porter_tokens[song_idx][i], snowball_tokens[song_idx][i], wordnet_tokens[song_idx][i]\n",
    "    if len(set((p, s, w))) != 1:\n",
    "        print(\"%16s | %16s | %16s | %16s |\" % (data['tokens'][song_idx][i], p, s, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, the results show that using any type of stemmer or lemmatizer seemed to detract from the words rather than help center them. These methods of word procession are not able to account for the colloqualisms that come from the language of rap. Therefore we will not proceed with using this for any word processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whats good nibba you fucking bitch nibba pussy\n",
      "Whats good nigga you fucking binch nigga pussy\n",
      "Whats good nigga you fucking bitch nigga boos\n",
      "Whats good nigga you duck bitch nigga pussy\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Whats good nigga you fucking bitch nigga pussy'\n",
    "\n",
    "bad_prefixes = ['nig', 'bitc', 'puss', 'fuc']\n",
    "replace_prefixes = ['nibba', 'binch', 'boos', 'duck']\n",
    "for pre, rep in zip(bad_prefixes, replace_prefixes):\n",
    "    match = r'({bad_prefix})(.*?)\\b'.format(bad_prefix=pre)\n",
    "    res = re.sub(match, rep, sentence)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad word regex\n"
     ]
    }
   ],
   "source": [
    "for x in sentence.split():\n",
    "#     if x.startswith(\"nig\"):\n",
    "#         print('bad word found')\n",
    "    if re.match(\"nig*\", x):\n",
    "        print(\"bad word regex\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
