{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto Reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Modules\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from pprint import pprint\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from math import log\n",
    "import re\n",
    "\n",
    "\n",
    "# NLTK Modules\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('tagsets')\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import chunk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Import Custom Modules\n",
    "from src.data_cleaner import *\n",
    "from src.dummy_words import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Scraping Program\n",
    "# !python src/web_scraper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intake Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>featured</th>\n",
       "      <th>rank</th>\n",
       "      <th>year</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>lyrics_state</th>\n",
       "      <th>song_id</th>\n",
       "      <th>lyrics_owner_id</th>\n",
       "      <th>primary_artist_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thrift Shop</td>\n",
       "      <td>Macklemore &amp; Ryan Lewis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>\"Hey, Macklemore, can we go thrift shopping?\"\\...</td>\n",
       "      <td>True</td>\n",
       "      <td>86538</td>\n",
       "      <td>3928</td>\n",
       "      <td>https://genius.com/artists/Macklemore-and-ryan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Can't Hold Us</td>\n",
       "      <td>Macklemore &amp; Ryan Lewis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>Hey, hey, hey\\nGood to see you\\nCome on, dude,...</td>\n",
       "      <td>True</td>\n",
       "      <td>57234</td>\n",
       "      <td>37383</td>\n",
       "      <td>https://genius.com/artists/Macklemore-and-ryan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Holy Grail</td>\n",
       "      <td>Jay Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>You'd take the clothes off my back and I'd let...</td>\n",
       "      <td>True</td>\n",
       "      <td>177832</td>\n",
       "      <td>104344</td>\n",
       "      <td>https://genius.com/artists/Jay-z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           song                    artist  featured  rank  year  \\\n",
       "0           0    Thrift Shop  Macklemore & Ryan Lewis        NaN     1  2000   \n",
       "1           1  Can't Hold Us  Macklemore & Ryan Lewis        NaN     2  2000   \n",
       "2           2     Holy Grail                    Jay Z        NaN     3  2000   \n",
       "\n",
       "                                              lyrics  lyrics_state  song_id  \\\n",
       "0  \"Hey, Macklemore, can we go thrift shopping?\"\\...          True    86538   \n",
       "1  Hey, hey, hey\\nGood to see you\\nCome on, dude,...          True    57234   \n",
       "2  You'd take the clothes off my back and I'd let...          True   177832   \n",
       "\n",
       "   lyrics_owner_id                                 primary_artist_url  \n",
       "0             3928  https://genius.com/artists/Macklemore-and-ryan...  \n",
       "1            37383  https://genius.com/artists/Macklemore-and-ryan...  \n",
       "2           104344                   https://genius.com/artists/Jay-z  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Data\n",
    "# data = pd.read_csv('data/data.csv')\n",
    "data = pd.read_csv('data/all_data.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 11 columns):\n",
      "Unnamed: 0            1000 non-null int64\n",
      "song                  1000 non-null object\n",
      "artist                1000 non-null object\n",
      "featured              0 non-null float64\n",
      "rank                  1000 non-null int64\n",
      "year                  1000 non-null int64\n",
      "lyrics                1000 non-null object\n",
      "lyrics_state          1000 non-null bool\n",
      "song_id               1000 non-null int64\n",
      "lyrics_owner_id       1000 non-null int64\n",
      "primary_artist_url    986 non-null object\n",
      "dtypes: bool(1), float64(1), int64(5), object(4)\n",
      "memory usage: 63.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[data['lyrics'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                            0\n",
       "song                                                        Thrift Shop\n",
       "artist                                         Macklemore & Ryan Lewis \n",
       "featured                                                            NaN\n",
       "rank                                                                  1\n",
       "year                                                               2000\n",
       "lyrics                \"Hey, Macklemore, can we go thrift shopping?\"\\...\n",
       "lyrics_state                                                       True\n",
       "song_id                                                           86538\n",
       "lyrics_owner_id                                                    3928\n",
       "primary_artist_url    https://genius.com/artists/Macklemore-and-ryan...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global Variables\n",
    "song_idx = 0\n",
    "data.iloc[song_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating pipeline for **indexing** song lyrics (document).\n",
    "\n",
    "This will lead to **indexing** which creates a **signature** (vector) for each document.\n",
    "\n",
    "Then, the **signatures** will be used for relating documents one to the other (and find out similar clusters of documents), or for mining underlying relations between concepts.\n",
    "\n",
    "<img src=\"media/text-pipeline.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bags of Words: Lyric String Example\n",
    "> With the intake lyric data, it seems that there needs to be a couple things cleaned. Casing, punctuation, and new-lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lyric String Passover 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Hey, Macklemore, can we go thrift shopping?\"\\nWhat what, what, what\\nWhat what, what, what\\nWhat what, what, what\\nWhat what, what, what\\nWhat what, what, what\\nBada, bada, bada doo da\\nWhat what, what, what\\nBada, bada, bada doo da\\nWhat what, what, what\\nBada, bada, bada doo da\\nBada, bada, bada doo da\\nBada, bada, bada doo da\\nBada, bada, bada doo da\\nBada, bada, bada doo da\\n\\nI\\'m gonna pop some tags\\nOnly got 20 dollars in my pocket\\nI\\'m, I\\'m, I\\'m huntin\\', lookin\\' for a come up\\nThis is fucking awesome\\n\\nNow\\nWalk into the club like, \"What up? I got a big cock\"\\nNah, I\\'m just pumped, I bought some shit from a thrift shop\\nIce on the fringe is so damn frosty\\nThe people like, \"Damn, that\\'s a cold ass honkey\"\\nRollin\\' in hella deep, headed to the mezzanine\\nDressed in '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data['lyrics'][song_idx]\n",
    "sample[:len(sample)//5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lyric String Passover 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean New-line breaks, but preserve periods\n",
    "data['lyrics'] = data['lyrics'].apply(lambda x: x.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Hey, Macklemore, can we go thrift shopping?\"',\n",
       " 'What what, what, what',\n",
       " 'What what, what, what',\n",
       " 'What what, what, what',\n",
       " 'What what, what, what',\n",
       " 'What what, what, what',\n",
       " 'Bada, bada, bada doo da',\n",
       " 'What what, what, what',\n",
       " 'Bada, bada, bada doo da',\n",
       " 'What what, what, what',\n",
       " 'Bada, bada, bada doo da',\n",
       " 'Bada, bada, bada doo da',\n",
       " 'Bada, bada, bada doo da',\n",
       " 'Bada, bada, bada doo da',\n",
       " 'Bada, bada, bada doo da',\n",
       " '',\n",
       " \"I'm gonna pop some tags\",\n",
       " 'Only got 20 dollars in my pocket',\n",
       " \"I'm, I'm, I'm huntin', lookin' for a come up\",\n",
       " 'This is fucking awesome']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data['lyrics'][song_idx]\n",
    "# Need to join due to splitting to list\n",
    "# \" \".join(sample[:len(sample)//5])\n",
    "sample[:len(sample)//5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lyric String Passover 3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Custom Text Cleaning Function\n",
    "data = clean_text(data, 'lyrics', 'lyrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey macklemore can we go thrift shopping',\n",
       " 'what what what what',\n",
       " 'what what what what',\n",
       " 'what what what what',\n",
       " 'what what what what',\n",
       " 'what what what what',\n",
       " 'bada bada bada doo da',\n",
       " 'what what what what',\n",
       " 'bada bada bada doo da',\n",
       " 'what what what what',\n",
       " 'bada bada bada doo da',\n",
       " 'bada bada bada doo da',\n",
       " 'bada bada bada doo da',\n",
       " 'bada bada bada doo da',\n",
       " 'bada bada bada doo da',\n",
       " '',\n",
       " 'im gonna pop some tags',\n",
       " 'only got 20 dollars in my pocket',\n",
       " 'im im im huntin lookin for a come up',\n",
       " 'this is fucking awesome']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data['lyrics'][song_idx]\n",
    "# Need to join due to splitting to list\n",
    "# \" \".join(sample[:len(sample)//5])\n",
    "sample[:len(sample)//5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 'document' Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Document Feature with Lyrics joined into one string (strips, negates whitespace)\n",
    "data['clean_text'] = data['lyrics'].apply(lambda x: \" \".join([i.strip() for i in x if i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey macklemore can we go thrift shopping what what what what what what what what what what what what what what what what what what what what bada bada bada doo da what what what what bada bada bada doo da what what what what bada bada bada doo da bada bada bada doo da bada bada bada doo da bada bada bada doo da bada bada bada doo da im gonna pop some tags only got 20 dollars in my pocket im im im huntin lookin for a come up this is fucking awesome now walk into the club like what up i got a big cock nah im just pumped i bought some shit from a thrift shop ice on the fringe is so damn frosty the people like damn thats a cold ass honkey rollin in hella deep headed to the mezzanine dressed in all pin'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data['clean_text'][song_idx]\n",
    "# Need to join due to splitting to list\n",
    "sample[:len(sample)//5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVE EXPLETIVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['clean_text'].apply(match_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sent Token Feature\n",
    "data['sentences'] = data['clean_text'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey macklemore can we go thrift shopping what what what what what what what what what what what what what what what what what what what what bada bada bada doo da what what what what bada bada bada doo da what what what what bada bada bada doo da bada bada bada doo da bada bada bada doo da bada bada bada doo da bada bada bada doo da im gonna pop some tags only got 20 dollars in my pocket im im im huntin lookin for a come up this is expletive_3 awesome now walk into the club like what up i got a big expletive_7 nah im just pumped i bought some expletive_4 from a thrift shop ice on the fringe is so damn frosty the people like damn thats a cold expletive_6 honkey rollin in hella deep headed to the mezzanine dressed in all pink cept my gator sexpletive_5 those are green draped in a leopard mink girl standin next to me probably shouldve washed this smells like r kelly sheets expletive_12 but expletive_4 it was 99 cents expletive_3 it coppin it washin it bout to go and get some compliments pexpletive_6 up on those moccasins someone else has been walkin in bummy and grungy expletive_3 it man i am stunting and flossin and saving my money and im hella happy thats a bargain expletive_1 ima take your grandpas style ima take your grandpas style no for real ask your grandpa can i have his handmedowns thank you velour jumpsuit and some house slippers dookie brown leather jacket that i found diggin they had a broken keyboard i bought a broken keyboard i bought a skeet blanket then i bought a knee board hello hello my ace man my mellow john wayne aint got nothing on my fringe game hell no i could take some pro wings make em cool sell those the sneaker heads would be like ah he got the velcros im gonna pop some tags only got 20 dollars in my pocket im im im huntin lookin for a come up this is expletive_3 awesome im gonna pop some tags only got 20 dollars in my pocket im im im huntin lookin for a come up this is expletive_3 awesome what you know about rockin a wolf on your noggin what you knowin about wearin a fur fox skin im diggin im diggin im searchin right through that luggage one mans trash thats another mans come up thank your granddad for donatin that plaid button up shirt cause right now im up in hurr stuntin im at the goodwill you can find me in the bins im not im not stuck on searchin in that section mens your grammy your auntie your mama your mammy ill take those flannel zebra jammies second hand and ill rock that motherexpletive_3 the builtin onesie with the socks on the motherexpletive_3 i hit the party and they stop in that motherexpletive_3 they be like oh that gucci thats hella tight im like yo thats 50 dollars for a tshirt limited edition lets do some simple addition 50 dollars for a tshirt thats just some ignorant expletive_1 expletive_4 i call that gettingswindledandpimped expletive_4 i call that getting tricked by business that shirts hella dough and having the same one as six other people in this club is a hella dont peep game come take a look through my telescope tryna get girls from a brand man you hella wont man you hella wont goodwill poppin tags yeah im gonna pop some tags only got 20 dollars in my pocket im im im huntin lookin for a come up this is expletive_3 awesome ill wear your granddads clothes i look incredible im in this bigexpletive_6 coat from that thrift shop down the road ill wear your granddads clothes damn right i look incredible now come on man im in this bigexpletive_6 coat bigexpletive_6 coat from that thrift shop down the road lets go come on im gonna pop some tags only got 20 dollars in my pocket im im im huntin lookin for a come up this is expletive_3 awesome is that your grandmas coat']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Sentences (truncated)\n",
    "data['sentences'][song_idx][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'macklemore', 'can', 'we', 'go', 'thrift', 'shopping', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what']\n"
     ]
    }
   ],
   "source": [
    "# Create tokens for each song\n",
    "data['tokens'] = data['clean_text'].apply(word_tokenize)\n",
    "print(data['tokens'][song_idx][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtration (Stop-words, punctiation, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'macklemore', 'can', 'we', 'go', 'thrift', 'shopping', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what']\n"
     ]
    }
   ],
   "source": [
    "# Filter Punctuation\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [i for i in x if i not in string.punctuation])\n",
    "print(data['tokens'][song_idx][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'macklemore', 'go', 'thrift', 'shopping', 'bada', 'bada', 'bada', 'doo', 'da', 'bada', 'bada', 'bada', 'doo', 'da', 'bada', 'bada', 'bada', 'doo', 'da']\n"
     ]
    }
   ],
   "source": [
    "# Filter Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words = re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", \" \".join(stop_words)).split() + ['im', 'ill']\n",
    "data['tokens_stop'] = data['tokens'].apply(lambda x: [i for i in x if i not in stop_words])\n",
    "print(data['tokens_stop'][song_idx][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Tokens to Lyric Unique Words (from whole document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n",
      "['hey', 'macklemore', 'can', 'we', 'go', 'thrift', 'shopping', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what']\n",
      "Set (from non tokens): \n",
      "['da', 'ask', 'jammies', 'what', 'lookin', 'wolf', 'coppin', 'dont', 'big', 'oh', 'find', 'jumpsuit', 'probably', 'i', 'next', 'me', 'telescope', 'he', 'auntie', 'knee']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens: \")\n",
    "print(data['tokens'][song_idx][:20])\n",
    "print(\"Set (from non tokens): \")\n",
    "print(list(set(data['clean_text'][song_idx].split()))[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem or Lemmatize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Porter, Snowball, WordNet Objects\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "# Get functions from each object\n",
    "porter_func = porter.stem\n",
    "snowball_func = snowball.stem\n",
    "wordnet_func = wordnet.lemmatize\n",
    "\n",
    "# Create lambda func to easily apply func to each token\n",
    "get_root = lambda tokens, func: [func(token) for token in tokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Tokens for each type of processor\n",
    "porter_tokens = data['tokens'].apply(lambda x: get_root(x, porter_func)) \n",
    "snowball_tokens = data['tokens'].apply(lambda x: get_root(x, snowball_func)) \n",
    "wordnet_tokens = data['tokens'].apply(lambda x: get_root(x, wordnet_func)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            WORD |           PORTER |         SNOWBALL |       LEMMATIZER |\n",
      "      macklemore |        macklemor |        macklemor |       macklemore |\n",
      "        shopping |             shop |             shop |         shopping |\n",
      "            only |             onli |             onli |             only |\n",
      "            this |              thi |             this |             this |\n",
      "         awesome |           awesom |           awesom |          awesome |\n",
      "          pumped |             pump |             pump |           pumped |\n",
      "          fringe |            fring |            fring |           fringe |\n",
      "          frosty |           frosti |           frosti |           frosty |\n",
      "          people |            peopl |            peopl |           people |\n",
      "           thats |             that |             that |            thats |\n",
      "          headed |             head |             head |           headed |\n",
      "       mezzanine |         mezzanin |         mezzanin |        mezzanine |\n",
      "         dressed |            dress |            dress |          dressed |\n",
      "          draped |            drape |            drape |           draped |\n",
      "        probably |          probabl |          probabl |         probably |\n",
      "        shouldve |          shouldv |          shouldv |         shouldve |\n",
      "          washed |             wash |             wash |           washed |\n",
      "            this |              thi |             this |             this |\n",
      "           kelly |            kelli |            kelli |            kelly |\n",
      "             was |               wa |              was |               wa |\n",
      "         someone |           someon |           someon |          someone |\n",
      "            else |              els |              els |             else |\n",
      "             has |               ha |              has |               ha |\n",
      "           bummy |            bummi |            bummi |            bummy |\n",
      "          grungy |           grungi |           grungi |           grungy |\n",
      "        stunting |            stunt |            stunt |         stunting |\n",
      "          saving |             save |             save |           saving |\n",
      "           happy |            happi |            happi |            happy |\n",
      "           thats |             that |             that |            thats |\n",
      "             his |               hi |              his |              his |\n",
      "     handmedowns |       handmedown |       handmedown |      handmedowns |\n",
      "           house |             hous |             hous |            house |\n",
      "          dookie |            dooki |            dooki |           dookie |\n",
      "           wayne |             wayn |             wayn |            wayne |\n",
      "         nothing |             noth |             noth |          nothing |\n",
      "          fringe |            fring |            fring |           fringe |\n",
      "            only |             onli |             onli |             only |\n",
      "            this |              thi |             this |             this |\n",
      "         awesome |           awesom |           awesom |          awesome |\n",
      "            only |             onli |             onli |             only |\n",
      "            this |              thi |             this |             this |\n",
      "         awesome |           awesom |           awesom |          awesome |\n",
      "         luggage |           luggag |           luggag |          luggage |\n",
      "           thats |             that |             that |            thats |\n",
      "         another |            anoth |            anoth |          another |\n",
      "           cause |             caus |             caus |            cause |\n",
      "        goodwill |          goodwil |          goodwil |         goodwill |\n",
      "          grammy |           grammi |           grammi |           grammy |\n",
      "          auntie |            aunti |            aunti |           auntie |\n",
      "           mammy |            mammi |            mammi |            mammy |\n",
      "         jammies |            jammi |            jammi |          jammies |\n",
      "          onesie |            onesi |            onesi |           onesie |\n",
      "           party |            parti |            parti |            party |\n",
      "           thats |             that |             that |            thats |\n",
      "           thats |             that |             that |            thats |\n",
      "         limited |            limit |            limit |          limited |\n",
      "         edition |             edit |             edit |          edition |\n",
      "          simple |            simpl |            simpl |           simple |\n",
      "        addition |            addit |            addit |         addition |\n",
      "           thats |             that |             that |            thats |\n",
      "        ignorant |            ignor |            ignor |         ignorant |\n",
      "gettingswindledandpimped | gettingswindledandpimp | gettingswindledandpimp | gettingswindledandpimped |\n",
      "         getting |              get |              get |          getting |\n",
      "         tricked |            trick |            trick |          tricked |\n",
      "        business |             busi |             busi |         business |\n",
      "          having |             have |             have |           having |\n",
      "              as |               as |               as |                a |\n",
      "          people |            peopl |            peopl |           people |\n",
      "            this |              thi |             this |             this |\n",
      "       telescope |         telescop |         telescop |        telescope |\n",
      "        goodwill |          goodwil |          goodwil |         goodwill |\n",
      "            only |             onli |             onli |             only |\n",
      "            this |              thi |             this |             this |\n",
      "         awesome |           awesom |           awesom |          awesome |\n",
      "         clothes |            cloth |            cloth |          clothes |\n",
      "      incredible |           incred |           incred |       incredible |\n",
      "            this |              thi |             this |             this |\n",
      "         clothes |            cloth |            cloth |          clothes |\n",
      "      incredible |           incred |           incred |       incredible |\n",
      "            this |              thi |             this |             this |\n",
      "            only |             onli |             onli |             only |\n",
      "            this |              thi |             this |             this |\n",
      "         awesome |           awesom |           awesom |          awesome |\n"
     ]
    }
   ],
   "source": [
    "## Print the stemmed and lemmatized words from the target document\n",
    "print(\"%16s | %16s | %16s | %16s |\" % (\"WORD\", \"PORTER\", \"SNOWBALL\", \"LEMMATIZER\"))\n",
    "for i in range(min(len(porter_tokens[song_idx]), len(snowball_tokens[song_idx]), len(wordnet_tokens[song_idx]))):\n",
    "    p, s, w = porter_tokens[song_idx][i], snowball_tokens[song_idx][i], wordnet_tokens[song_idx][i]\n",
    "    if len(set((p, s, w))) != 1:\n",
    "        print(\"%16s | %16s | %16s | %16s |\" % (data['tokens'][song_idx][i], p, s, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, the results show that using any type of stemmer or lemmatizer seemed to detract from the words rather than help center them. These methods of word procession are not able to account for the colloqualisms that come from the language of rap. Therefore we will not proceed with using this for any word processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Features Added\n",
    "\n",
    "Now that the tokens are extracted from the lyric set, it's time to create a new feature with the SET of tokens for ease of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['da',\n",
       " 'ask',\n",
       " 'jammies',\n",
       " 'what',\n",
       " 'lookin',\n",
       " 'wolf',\n",
       " 'coppin',\n",
       " 'dont',\n",
       " 'big',\n",
       " 'oh']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Token Set Feature\n",
    "data['token_set'] = data['tokens'].apply(lambda x: list(set(x)))\n",
    "data['token_set'][song_idx][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams\n",
    "\n",
    "It might be useful to see if N-Grams would give us a better list of tokens, since most rap lyrics involve heavy use of consecutive and connected words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hey', 'macklemore'),\n",
       " ('macklemore', 'can'),\n",
       " ('can', 'we'),\n",
       " ('we', 'go'),\n",
       " ('go', 'thrift'),\n",
       " ('thrift', 'shopping'),\n",
       " ('shopping', 'what'),\n",
       " ('what', 'what'),\n",
       " ('what', 'what'),\n",
       " ('what', 'what')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(data['tokens'][song_idx], 2))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['da', 'NN'],\n",
       "       ['ask', 'NN'],\n",
       "       ['jammies', 'NNS'],\n",
       "       ['what', 'WP'],\n",
       "       ['lookin', 'VBP'],\n",
       "       ['wolf', 'NN'],\n",
       "       ['coppin', 'NN'],\n",
       "       ['dont', 'NN'],\n",
       "       ['big', 'JJ'],\n",
       "       ['oh', 'NN']], dtype='<U7')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimalize Tag Functions\n",
    "pos_tagger = nltk.pos_tag\n",
    "explain_tag = nltk.help.upenn_tagset\n",
    "\n",
    "# Get Sample Tags\n",
    "tag_sample = np.array(pos_tagger(set(data['tokens'][song_idx]))[:10])\n",
    "tag_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "WORDS: ['big']\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "WORDS: ['da', 'ask', 'wolf', 'coppin', 'dont', 'oh']\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "WORDS: ['jammies']\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "WORDS: ['lookin']\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WORDS: ['what']\n"
     ]
    }
   ],
   "source": [
    "# Split Words and Tags\n",
    "words, tags = tag_sample[:, 0], tag_sample[:, 1]\n",
    "\n",
    "# Create DF to Groupby\n",
    "tag_df = pd.DataFrame({'words':words, 'tags':tags})\n",
    "grouped_tags = tag_df.groupby('tags')\n",
    "\n",
    "for x in grouped_tags:\n",
    "    word = x[1]['words']\n",
    "    explain_tag(x[0])\n",
    "    print(f'WORDS: {word.tolist()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using POS Tagging seems to give more insight to the words and what they represent, but similarly to the Stemmers/Lemmatizers, they seem to also miscategorize things. The word 'patek' is actually a brand reference to Patek Watches, a luxury watch brand, and is not a verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Extra Column\n",
    "data = data.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have to drop 2000-2013 due to bad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_years = range(2000, 2013)\n",
    "for year in bad_years:\n",
    "    data = data[data['year'] != year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>featured</th>\n",
       "      <th>rank</th>\n",
       "      <th>year</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>lyrics_state</th>\n",
       "      <th>song_id</th>\n",
       "      <th>lyrics_owner_id</th>\n",
       "      <th>primary_artist_url</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_stop</th>\n",
       "      <th>token_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thrift Shop</td>\n",
       "      <td>Macklemore &amp; Ryan Lewis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>[hey macklemore can we go thrift shopping, wha...</td>\n",
       "      <td>True</td>\n",
       "      <td>86538</td>\n",
       "      <td>3928</td>\n",
       "      <td>https://genius.com/artists/Macklemore-and-ryan...</td>\n",
       "      <td>hey macklemore can we go thrift shopping what ...</td>\n",
       "      <td>[hey macklemore can we go thrift shopping what...</td>\n",
       "      <td>[hey, macklemore, can, we, go, thrift, shoppin...</td>\n",
       "      <td>[hey, macklemore, go, thrift, shopping, bada, ...</td>\n",
       "      <td>[da, ask, jammies, what, lookin, wolf, coppin,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can't Hold Us</td>\n",
       "      <td>Macklemore &amp; Ryan Lewis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>[hey hey hey, good to see you, come on dude le...</td>\n",
       "      <td>True</td>\n",
       "      <td>57234</td>\n",
       "      <td>37383</td>\n",
       "      <td>https://genius.com/artists/Macklemore-and-ryan...</td>\n",
       "      <td>hey hey hey good to see you come on dude lets ...</td>\n",
       "      <td>[hey hey hey good to see you come on dude lets...</td>\n",
       "      <td>[hey, hey, hey, good, to, see, you, come, on, ...</td>\n",
       "      <td>[hey, hey, hey, good, see, come, dude, lets, g...</td>\n",
       "      <td>[pounds, gone, humble, humility, what, city, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Holy Grail</td>\n",
       "      <td>Jay Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>[youd take the clothes off my back and id let ...</td>\n",
       "      <td>True</td>\n",
       "      <td>177832</td>\n",
       "      <td>104344</td>\n",
       "      <td>https://genius.com/artists/Jay-z</td>\n",
       "      <td>youd take the clothes off my back and id let y...</td>\n",
       "      <td>[youd take the clothes off my back and id let ...</td>\n",
       "      <td>[youd, take, the, clothes, off, my, back, and,...</td>\n",
       "      <td>[take, clothes, back, id, let, steal, food, ri...</td>\n",
       "      <td>[care, though, blame, body, play, twice, dying...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Started From The Bottom</td>\n",
       "      <td>Drake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>[started, zombie on the track, , started from ...</td>\n",
       "      <td>True</td>\n",
       "      <td>113898</td>\n",
       "      <td>103043</td>\n",
       "      <td>https://genius.com/artists/Drake</td>\n",
       "      <td>started zombie on the track started from the b...</td>\n",
       "      <td>[started zombie on the track started from the ...</td>\n",
       "      <td>[started, zombie, on, the, track, started, fro...</td>\n",
       "      <td>[started, zombie, track, started, bottom, star...</td>\n",
       "      <td>[friends, kept, give, expletive_0, dont, keys,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feel This Moment</td>\n",
       "      <td>Pitbull</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2013</td>\n",
       "      <td>[ask for money and get advice, ask for advice ...</td>\n",
       "      <td>True</td>\n",
       "      <td>115843</td>\n",
       "      <td>174558</td>\n",
       "      <td>https://genius.com/artists/Pitbull</td>\n",
       "      <td>ask for money and get advice ask for advice ge...</td>\n",
       "      <td>[ask for money and get advice ask for advice g...</td>\n",
       "      <td>[ask, for, money, and, get, advice, ask, for, ...</td>\n",
       "      <td>[ask, money, get, advice, ask, advice, get, mo...</td>\n",
       "      <td>[ask, twice, sense, what, give, dont, os, i, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>Press</td>\n",
       "      <td>Cardi B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46</td>\n",
       "      <td>2019</td>\n",
       "      <td>[monstas gon tear it up, bardi, woo yeah, bitc...</td>\n",
       "      <td>True</td>\n",
       "      <td>4191420</td>\n",
       "      <td>8117646</td>\n",
       "      <td>https://genius.com/artists/Cardi-b</td>\n",
       "      <td>monstas gon tear it up bardi woo yeah expletiv...</td>\n",
       "      <td>[monstas gon tear it up bardi woo yeah expleti...</td>\n",
       "      <td>[monstas, gon, tear, it, up, bardi, woo, yeah,...</td>\n",
       "      <td>[monstas, gon, tear, bardi, woo, yeah, expleti...</td>\n",
       "      <td>[wrist, ask, ready, biggest, anybody, brr, wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>Backin' It Up</td>\n",
       "      <td>Pardison Fontaine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47</td>\n",
       "      <td>2019</td>\n",
       "      <td>[ahhh, cardi, turn around fuck it all the way ...</td>\n",
       "      <td>True</td>\n",
       "      <td>3970189</td>\n",
       "      <td>104344</td>\n",
       "      <td>https://genius.com/artists/Pardison-fontaine</td>\n",
       "      <td>ahhh cardi turn around expletive_3 it all the ...</td>\n",
       "      <td>[ahhh cardi turn around expletive_3 it all the...</td>\n",
       "      <td>[ahhh, cardi, turn, around, expletive_3, it, a...</td>\n",
       "      <td>[ahhh, cardi, turn, around, expletive_3, way, ...</td>\n",
       "      <td>[saran, lame, facetime, wrap, expletive_12, wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>Twerk</td>\n",
       "      <td>City Girls</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>2019</td>\n",
       "      <td>[i want a slim fine woman with some twerk with...</td>\n",
       "      <td>True</td>\n",
       "      <td>4080406</td>\n",
       "      <td>1603328</td>\n",
       "      <td>https://genius.com/artists/City-girls</td>\n",
       "      <td>i want a slim fine woman with some twerk with ...</td>\n",
       "      <td>[i want a slim fine woman with some twerk with...</td>\n",
       "      <td>[i, want, a, slim, fine, woman, with, some, tw...</td>\n",
       "      <td>[want, slim, fine, woman, twerk, throw, twerk,...</td>\n",
       "      <td>[stiff, dupri, catch, what, city, lookin, is, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>Time</td>\n",
       "      <td>NF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>2019</td>\n",
       "      <td>[even if we both break down tonight, and you s...</td>\n",
       "      <td>True</td>\n",
       "      <td>4693540</td>\n",
       "      <td>3375166</td>\n",
       "      <td>https://genius.com/artists/Nf</td>\n",
       "      <td>even if we both break down toexpletive_0 and y...</td>\n",
       "      <td>[even if we both break down toexpletive_0 and ...</td>\n",
       "      <td>[even, if, we, both, break, down, toexpletive_...</td>\n",
       "      <td>[even, break, toexpletive_0, say, hate, go, be...</td>\n",
       "      <td>[nervous, gone, what, soon, lookin, pride, don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>Mixed Personalities</td>\n",
       "      <td>YNW Melly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>2019</td>\n",
       "      <td>[because i am hard you will not like me, , say...</td>\n",
       "      <td>True</td>\n",
       "      <td>4230193</td>\n",
       "      <td>104344</td>\n",
       "      <td>https://genius.com/artists/Ynw-melly</td>\n",
       "      <td>because i am hard you will not like me say you...</td>\n",
       "      <td>[because i am hard you will not like me say yo...</td>\n",
       "      <td>[because, i, am, hard, you, will, not, like, m...</td>\n",
       "      <td>[hard, like, say, want, someone, say, want, so...</td>\n",
       "      <td>[glock, thatll, vault, catch, lithanol, give, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        song                    artist  featured  rank  year  \\\n",
       "0                Thrift Shop  Macklemore & Ryan Lewis        NaN     1  2013   \n",
       "1              Can't Hold Us  Macklemore & Ryan Lewis        NaN     2  2013   \n",
       "2                 Holy Grail                    Jay Z        NaN     3  2013   \n",
       "3    Started From The Bottom                     Drake       NaN     4  2013   \n",
       "4           Feel This Moment                  Pitbull        NaN     5  2013   \n",
       "..                       ...                       ...       ...   ...   ...   \n",
       "345                    Press                   Cardi B       NaN    46  2019   \n",
       "346            Backin' It Up        Pardison Fontaine        NaN    47  2019   \n",
       "347                    Twerk               City Girls        NaN    48  2019   \n",
       "348                     Time                        NF       NaN    49  2019   \n",
       "349      Mixed Personalities                YNW Melly        NaN    50  2019   \n",
       "\n",
       "                                                lyrics  lyrics_state  song_id  \\\n",
       "0    [hey macklemore can we go thrift shopping, wha...          True    86538   \n",
       "1    [hey hey hey, good to see you, come on dude le...          True    57234   \n",
       "2    [youd take the clothes off my back and id let ...          True   177832   \n",
       "3    [started, zombie on the track, , started from ...          True   113898   \n",
       "4    [ask for money and get advice, ask for advice ...          True   115843   \n",
       "..                                                 ...           ...      ...   \n",
       "345  [monstas gon tear it up, bardi, woo yeah, bitc...          True  4191420   \n",
       "346  [ahhh, cardi, turn around fuck it all the way ...          True  3970189   \n",
       "347  [i want a slim fine woman with some twerk with...          True  4080406   \n",
       "348  [even if we both break down tonight, and you s...          True  4693540   \n",
       "349  [because i am hard you will not like me, , say...          True  4230193   \n",
       "\n",
       "     lyrics_owner_id                                 primary_artist_url  \\\n",
       "0               3928  https://genius.com/artists/Macklemore-and-ryan...   \n",
       "1              37383  https://genius.com/artists/Macklemore-and-ryan...   \n",
       "2             104344                   https://genius.com/artists/Jay-z   \n",
       "3             103043                   https://genius.com/artists/Drake   \n",
       "4             174558                 https://genius.com/artists/Pitbull   \n",
       "..               ...                                                ...   \n",
       "345          8117646                 https://genius.com/artists/Cardi-b   \n",
       "346           104344       https://genius.com/artists/Pardison-fontaine   \n",
       "347          1603328              https://genius.com/artists/City-girls   \n",
       "348          3375166                      https://genius.com/artists/Nf   \n",
       "349           104344               https://genius.com/artists/Ynw-melly   \n",
       "\n",
       "                                            clean_text  \\\n",
       "0    hey macklemore can we go thrift shopping what ...   \n",
       "1    hey hey hey good to see you come on dude lets ...   \n",
       "2    youd take the clothes off my back and id let y...   \n",
       "3    started zombie on the track started from the b...   \n",
       "4    ask for money and get advice ask for advice ge...   \n",
       "..                                                 ...   \n",
       "345  monstas gon tear it up bardi woo yeah expletiv...   \n",
       "346  ahhh cardi turn around expletive_3 it all the ...   \n",
       "347  i want a slim fine woman with some twerk with ...   \n",
       "348  even if we both break down toexpletive_0 and y...   \n",
       "349  because i am hard you will not like me say you...   \n",
       "\n",
       "                                             sentences  \\\n",
       "0    [hey macklemore can we go thrift shopping what...   \n",
       "1    [hey hey hey good to see you come on dude lets...   \n",
       "2    [youd take the clothes off my back and id let ...   \n",
       "3    [started zombie on the track started from the ...   \n",
       "4    [ask for money and get advice ask for advice g...   \n",
       "..                                                 ...   \n",
       "345  [monstas gon tear it up bardi woo yeah expleti...   \n",
       "346  [ahhh cardi turn around expletive_3 it all the...   \n",
       "347  [i want a slim fine woman with some twerk with...   \n",
       "348  [even if we both break down toexpletive_0 and ...   \n",
       "349  [because i am hard you will not like me say yo...   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    [hey, macklemore, can, we, go, thrift, shoppin...   \n",
       "1    [hey, hey, hey, good, to, see, you, come, on, ...   \n",
       "2    [youd, take, the, clothes, off, my, back, and,...   \n",
       "3    [started, zombie, on, the, track, started, fro...   \n",
       "4    [ask, for, money, and, get, advice, ask, for, ...   \n",
       "..                                                 ...   \n",
       "345  [monstas, gon, tear, it, up, bardi, woo, yeah,...   \n",
       "346  [ahhh, cardi, turn, around, expletive_3, it, a...   \n",
       "347  [i, want, a, slim, fine, woman, with, some, tw...   \n",
       "348  [even, if, we, both, break, down, toexpletive_...   \n",
       "349  [because, i, am, hard, you, will, not, like, m...   \n",
       "\n",
       "                                           tokens_stop  \\\n",
       "0    [hey, macklemore, go, thrift, shopping, bada, ...   \n",
       "1    [hey, hey, hey, good, see, come, dude, lets, g...   \n",
       "2    [take, clothes, back, id, let, steal, food, ri...   \n",
       "3    [started, zombie, track, started, bottom, star...   \n",
       "4    [ask, money, get, advice, ask, advice, get, mo...   \n",
       "..                                                 ...   \n",
       "345  [monstas, gon, tear, bardi, woo, yeah, expleti...   \n",
       "346  [ahhh, cardi, turn, around, expletive_3, way, ...   \n",
       "347  [want, slim, fine, woman, twerk, throw, twerk,...   \n",
       "348  [even, break, toexpletive_0, say, hate, go, be...   \n",
       "349  [hard, like, say, want, someone, say, want, so...   \n",
       "\n",
       "                                             token_set  \n",
       "0    [da, ask, jammies, what, lookin, wolf, coppin,...  \n",
       "1    [pounds, gone, humble, humility, what, city, g...  \n",
       "2    [care, though, blame, body, play, twice, dying...  \n",
       "3    [friends, kept, give, expletive_0, dont, keys,...  \n",
       "4    [ask, twice, sense, what, give, dont, os, i, w...  \n",
       "..                                                 ...  \n",
       "345  [wrist, ask, ready, biggest, anybody, brr, wha...  \n",
       "346  [saran, lame, facetime, wrap, expletive_12, wh...  \n",
       "347  [stiff, dupri, catch, what, city, lookin, is, ...  \n",
       "348  [nervous, gone, what, soon, lookin, pride, don...  \n",
       "349  [glock, thatll, vault, catch, lithanol, give, ...  \n",
       "\n",
       "[350 rows x 15 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reset_index(drop=True, inplace=True)\n",
    "# data = data.drop(columns=['level_0'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, export data to clean_data.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_pickle('data/clean_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('data/all_clean_data.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
